[{"role": "system", "content": "You are a helpful AI bot that answers questions for a user. Keep your response short and direct"}, {"role": "user", "content": "\nThe best thing to do in San Francisco is eat a sandwich and sit in Dolores Park on a sunny day.\n September 2007In high school I decided I was going to study philosophy in college.\nI had several motives, some more honorable than others.  One of the\nless honorable was to shock people.  College was regarded as job\ntraining where I grew up, so studying philosophy seemed an impressively\nimpractical thing to do.  Sort of like slashing holes in your clothes\nor putting a safety pin through your ear, which were other forms\nof impressive impracticality then just coming into fashion.But I had some more honest motives as well.  I thought studying\nphilosophy would be a shortcut straight to wisdom.  All the people\nmajoring in other things would just end up with a bunch of domain\nknowledge.  I would be learning what was really what.I'd tried to read a few philosophy books.  Not recent ones; you\nwouldn't find those in our high school library.  But I tried to\nread Plato and Aristotle.  I doubt I believed I understood them,\nbut they sounded like they were talking about something important.\nI assumed I'd learn what in college.The summer before senior year I took some college classes.  I learned\na lot in the calculus class, but I didn't learn much in Philosophy\n101.  And yet my plan to study philosophy remained intact.  It was\nmy fault I hadn't learned anything.  I hadn't read the books we\nwere assigned carefully enough.  I'd give Berkeley's Principles\nof Human Knowledge another shot in college.  Anything so admired\nand so difficult to read must have something in it, if one could\nonly figure out what.Twenty-six years later, I still don't understand Berkeley.  I have\na nice edition of his collected works.  Will I ever read it?  Seems\nunlikely.The difference between then and now is that now I understand why\nBerkeley is probably not worth trying to understand.  I think I see\nnow what went wrong with philosophy, and how we might fix it.WordsI did end up being a philosophy major for most of college.  It\ndidn't work out as I'd hoped.  I didn't learn any magical truths\ncompared to which everything else was mere domain knowledge.  But\nI do at least know now why I didn't.  Philosophy doesn't really\nhave a subject matter in the way math or history or most other\nuniversity subjects do.  There is no core of knowledge one must\nmaster.  The closest you come to that is a knowledge of what various\nindividual philosophers have said about different topics over the\nyears.  Few were sufficiently correct that people have forgotten\nwho discovered what they discovered.Formal logic has some subject matter. I took several classes in\nlogic.  I don't know if I learned anything from them.\n[1]\nIt does seem to me very important to be able to flip ideas around in\none's head: to see when two ideas don't fully cover the space of\npossibilities, or when one idea is the same as another but with a\ncouple things changed.  But did studying logic teach me the importance\nof thinking this way, or make me any better at it?  I don't know.There are things I know I learned from studying philosophy.  The\nmost dramatic I learned immediately, in the first semester of\nfreshman year, in a class taught by Sydney Shoemaker.  I learned\nthat I don't exist.  I am (and you are) a collection of cells that\nlurches around driven by various forces, and calls itself I.  But\nthere's no central, indivisible thing that your identity goes with.\nYou could conceivably lose half your brain and live.  Which means\nyour brain could conceivably be split into two halves and each\ntransplanted into different bodies.  Imagine waking up after such\nan operation.  You have to imagine being two people.The real lesson here is that the concepts we use in everyday life\nare fuzzy, and break down if pushed too hard.  Even a concept as\ndear to us as I.  It took me a while to grasp this, but when I\ndid it was fairly sudden, like someone in the nineteenth century\ngrasping evolution and realizing the story of creation they'd been\ntold as a child was all wrong. \n[2]\nOutside of math there's a limit\nto how far you can push words; in fact, it would not be a bad\ndefinition of math to call it the study of terms that have precise\nmeanings.  Everyday words are inherently imprecise.  They work well\nenough in everyday life that you don't notice.  Words seem to work,\njust as Newtonian physics seems to.  But you can always make them\nbreak if you push them far enough.I would say that this has been, unfortunately for philosophy, the\ncentral fact of philosophy.  Most philosophical debates are not\nmerely afflicted by but driven by confusions over words.  Do we\nhave free will?  Depends what you mean by \"free.\" Do abstract ideas\nexist?  Depends what you mean by \"exist.\"Wittgenstein is popularly credited with the idea that most philosophical\ncontroversies are due to confusions over language.  I'm not sure\nhow much credit to give him.  I suspect a lot of people realized\nthis, but reacted simply by not studying philosophy, rather than\nbecoming philosophy professors.How did things get this way?  Can something people have spent\nthousands of years studying really be a waste of time?  Those are\ninteresting questions.  In fact, some of the most interesting\nquestions you can ask about philosophy.  The most valuable way to\napproach the current philosophical tradition may be neither to get\nlost in pointless speculations like Berkeley, nor to shut them down\nlike Wittgenstein, but to study it as an example of reason gone\nwrong.HistoryWestern philosophy really begins with Socrates, Plato, and Aristotle.\nWhat we know of their predecessors comes from fragments and references\nin later works; their doctrines could be described as speculative\ncosmology that occasionally strays into analysis.  Presumably they\nwere driven by whatever makes people in every other society invent\ncosmologies.\n[3]With Socrates, Plato, and particularly Aristotle, this tradition\nturned a corner.  There started to be a lot more analysis.  I suspect\nPlato and Aristotle were encouraged in this by progress in math.\nMathematicians had by then shown that you could figure things out\nin a much more conclusive way than by making up fine sounding stories\nabout them.  \n[4]People talk so much about abstractions now that we don't realize\nwhat a leap it must have been when they first started to.  It was\npresumably many thousands of years between when people first started\ndescribing things as hot or cold and when someone asked \"what is\nheat?\"  No doubt it was a very gradual process.  We don't know if\nPlato or Aristotle were the first to ask any of the questions they\ndid.  But their works are the oldest we have that do this on a large\nscale, and there is a freshness (not to say naivete) about them\nthat suggests some of the questions they asked were new to them,\nat least.Aristotle in particular reminds me of the phenomenon that happens\nwhen people discover something new, and are so excited by it that\nthey race through a huge percentage of the newly discovered territory\nin one lifetime.  If so, that's evidence of how new this kind of\nthinking was. \n[5]This is all to explain how Plato and Aristotle can be very impressive\nand yet naive and mistaken.  It was impressive even to ask the\nquestions they did.  That doesn't mean they always came up with\ngood answers.  It's not considered insulting to say that ancient\nGreek mathematicians were naive in some respects, or at least lacked\nsome concepts that would have made their lives easier.  So I hope\npeople will not be too offended if I propose that ancient philosophers\nwere similarly naive.  In particular, they don't seem to have fully\ngrasped what I earlier called the central fact of philosophy: that\nwords break if you push them too far.\"Much to the surprise of the builders of the first digital computers,\"\nRod Brooks wrote, \"programs written for them usually did not work.\"\n[6]\nSomething similar happened when people first started trying\nto talk about abstractions.  Much to their surprise, they didn't\narrive at answers they agreed upon.  In fact, they rarely seemed\nto arrive at answers at all.They were in effect arguing about artifacts induced by sampling at\ntoo low a resolution.The proof of how useless some of their answers turned out to be is\nhow little effect they have.  No one after reading Aristotle's\nMetaphysics does anything differently as a result.\n[7]Surely I'm not claiming that ideas have to have practical applications\nto be interesting?  No, they may not have to.  Hardy's boast that\nnumber theory had no use whatsoever wouldn't disqualify it.  But\nhe turned out to be mistaken.  In fact, it's suspiciously hard to\nfind a field of math that truly has no practical use.  And Aristotle's\nexplanation of the ultimate goal of philosophy in Book A of the\nMetaphysics implies that philosophy should be useful too.Theoretical KnowledgeAristotle's goal was to find the most general of general principles.\nThe examples he gives are convincing: an ordinary worker builds\nthings a certain way out of habit; a master craftsman can do more\nbecause he grasps the underlying principles.  The trend is clear:\nthe more general the knowledge, the more admirable it is.  But then\nhe makes a mistake\u2014possibly the most important mistake in the\nhistory of philosophy.  He has noticed that theoretical knowledge\nis often acquired for its own sake, out of curiosity, rather than\nfor any practical need.  So he proposes there are two kinds of\ntheoretical knowledge: some that's useful in practical matters and\nsome that isn't.  Since people interested in the latter are interested\nin it for its own sake, it must be more noble.  So he sets as his\ngoal in the Metaphysics the exploration of knowledge that has no\npractical use.  Which means no alarms go off when he takes on grand\nbut vaguely understood questions and ends up getting lost in a sea\nof words.His mistake was to confuse motive and result.  Certainly, people\nwho want a deep understanding of something are often driven by\ncuriosity rather than any practical need.  But that doesn't mean\nwhat they end up learning is useless.  It's very valuable in practice\nto have a deep understanding of what you're doing; even if you're\nnever called on to solve advanced problems, you can see shortcuts\nin the solution of simple ones, and your knowledge won't break down\nin edge cases, as it would if you were relying on formulas you\ndidn't understand.  Knowledge is power.  That's what makes theoretical\nknowledge prestigious.  It's also what causes smart people to be\ncurious about certain things and not others; our DNA is not so\ndisinterested as we might think.So while ideas don't have to have immediate practical applications\nto be interesting, the kinds of things we find interesting will\nsurprisingly often turn out to have practical applications.The reason Aristotle didn't get anywhere in the Metaphysics was\npartly that he set off with contradictory aims: to explore the most\nabstract ideas, guided by the assumption that they were useless.\nHe was like an explorer looking for a territory to the north of\nhim, starting with the assumption that it was located to the south.And since his work became the map used by generations of future\nexplorers, he sent them off in the wrong direction as well. \n[8]\nPerhaps worst of all, he protected them from both the criticism of\noutsiders and the promptings of their own inner compass by establishing\nthe principle that the most noble sort of theoretical knowledge had\nto be useless.The Metaphysics is mostly a failed experiment.  A few ideas from\nit turned out to be worth keeping; the bulk of it has had no effect\nat all.  The Metaphysics is among the least read of all famous\nbooks.  It's not hard to understand the way Newton's Principia\nis, but the way a garbled message is.Arguably it's an interesting failed experiment.  But unfortunately\nthat was not the conclusion Aristotle's successors derived from\nworks like the Metaphysics. \n[9]\nSoon after, the western world\nfell on intellectual hard times.  Instead of version 1s to be\nsuperseded, the works of Plato and Aristotle became revered texts\nto be mastered and discussed.  And so things remained for a shockingly\nlong time.  It was not till around 1600 (in Europe, where the center\nof gravity had shifted by then) that one found people confident\nenough to treat Aristotle's work as a catalog of mistakes.  And\neven then they rarely said so outright.If it seems surprising that the gap was so long, consider how little\nprogress there was in math between Hellenistic times and the\nRenaissance.In the intervening years an unfortunate idea took hold:  that it\nwas not only acceptable to produce works like the Metaphysics,\nbut that it was a particularly prestigious line of work, done by a\nclass of people called philosophers.  No one thought to go back and\ndebug Aristotle's motivating argument.  And so instead of correcting\nthe problem Aristotle discovered by falling into it\u2014that you can\neasily get lost if you talk too loosely about very abstract ideas\u2014they \ncontinued to fall into it.The SingularityCuriously, however, the works they produced continued to attract\nnew readers.  Traditional philosophy occupies a kind of singularity\nin this respect.  If you write in an unclear way about big ideas,\nyou produce something that seems tantalizingly attractive to\ninexperienced but intellectually ambitious students.  Till one knows\nbetter, it's hard to distinguish something that's hard to understand\nbecause the writer was unclear in his own mind from something like\na mathematical proof that's hard to understand because the ideas\nit represents are hard to understand.  To someone who hasn't learned\nthe difference, traditional philosophy seems extremely attractive:\nas hard (and therefore impressive) as math, yet broader in scope.\nThat was what lured me in as a high school student.This singularity is even more singular in having its own defense\nbuilt in.  When things are hard to understand, people who suspect\nthey're nonsense generally keep quiet.  There's no way to prove a\ntext is meaningless.  The closest you can get is to show that the\nofficial judges of some class of texts can't distinguish them from\nplacebos. \n[10]And so instead of denouncing philosophy, most people who suspected\nit was a waste of time just studied other things.  That alone is\nfairly damning evidence, considering philosophy's claims.  It's\nsupposed to be about the ultimate truths. Surely all smart people\nwould be interested in it, if it delivered on that promise.Because philosophy's flaws turned away the sort of people who might\nhave corrected them, they tended to be self-perpetuating.  Bertrand\nRussell wrote in a letter in 1912:\n\n  Hitherto the people attracted to philosophy have been mostly those\n  who loved the big generalizations, which were all wrong, so that\n  few people with exact minds have taken up the subject.\n[11]\n\nHis response was to launch Wittgenstein at it, with dramatic results.I think Wittgenstein deserves to be famous not for the discovery\nthat most previous philosophy was a waste of time, which judging\nfrom the circumstantial evidence must have been made by every smart\nperson who studied a little philosophy and declined to pursue it\nfurther, but for how he acted in response.\n[12]\nInstead of quietly\nswitching to another field, he made a fuss, from inside.  He was\nGorbachev.The field of philosophy is still shaken from the fright Wittgenstein\ngave it. \n[13]\nLater in life he spent a lot of time talking about\nhow words worked.  Since that seems to be allowed, that's what a\nlot of philosophers do now.  Meanwhile, sensing a vacuum in the\nmetaphysical speculation department, the people who used to do\nliterary criticism have been edging Kantward, under new names like\n\"literary theory,\" \"critical theory,\" and when they're feeling\nambitious, plain \"theory.\"  The writing is the familiar word salad:\n\n  Gender is not like some of the other grammatical modes which\n  express precisely a mode of conception without any reality that\n  corresponds to the conceptual mode, and consequently do not express\n  precisely something in reality by which the intellect could be\n  moved to conceive a thing the way it does, even where that motive\n  is not something in the thing as such.\n  [14]\n\nThe singularity I've described is not going away.  There's a market\nfor writing that sounds impressive and can't be disproven. There\nwill always be both supply and demand.  So if one group abandons\nthis territory, there will always be others ready to occupy it.A ProposalWe may be able to do better.  Here's an intriguing possibility.\nPerhaps we should do what Aristotle meant to do, instead of what\nhe did.  The goal he announces in the Metaphysics seems one worth\npursuing: to discover the most general truths.  That sounds good.\nBut instead of trying to discover them because they're useless,\nlet's try to discover them because they're useful.I propose we try again, but that we use that heretofore despised\ncriterion, applicability, as a guide to keep us from wondering\noff into a swamp of abstractions.  Instead of trying to answer the\nquestion:\n\n  What are the most general truths?\n\nlet's try to answer the question\n\n  Of all the useful things we can say, which are the most general?\n\nThe test of utility I propose is whether we cause people who read\nwhat we've written to do anything differently afterward.  Knowing\nwe have to give definite (if implicit) advice will keep us from\nstraying beyond the resolution of the words we're using.The goal is the same as Aristotle's; we just approach it from a\ndifferent direction.As an example of a useful, general idea, consider that of the\ncontrolled experiment.  There's an idea that has turned out to be\nwidely applicable.  Some might say it's part of science, but it's\nnot part of any specific science; it's literally meta-physics (in\nour sense of \"meta\").   The idea of evolution is another. It turns\nout to have quite broad applications\u2014for example, in genetic\nalgorithms and even product design.  Frankfurt's distinction between\nlying and bullshitting seems a promising recent example.\n[15]These seem to me what philosophy should look like: quite general\nobservations that would cause someone who understood them to do\nsomething differently.Such observations will necessarily be about things that are imprecisely\ndefined.  Once you start using words with precise meanings, you're\ndoing math.  So starting from utility won't entirely solve the\nproblem I described above\u2014it won't flush out the metaphysical\nsingularity.  But it should help.  It gives people with good\nintentions a new roadmap into abstraction.  And they may thereby\nproduce things that make the writing of the people with bad intentions\nlook bad by comparison.One drawback of this approach is that it won't produce the sort of\nwriting that gets you tenure.  And not just because it's not currently\nthe fashion.  In order to get tenure in any field you must not\narrive at conclusions that members of tenure committees can disagree\nwith.  In practice there are two kinds of solutions to this problem.\nIn math and the sciences, you can prove what you're saying, or at\nany rate adjust your conclusions so you're not claiming anything\nfalse (\"6 of 8 subjects had lower blood pressure after the treatment\").\nIn the humanities you can either avoid drawing any definite conclusions\n(e.g. conclude that an issue is a complex one), or draw conclusions\nso narrow that no one cares enough to disagree with you.The kind of philosophy I'm advocating won't be able to take either\nof these routes.  At best you'll be able to achieve the essayist's\nstandard of proof, not the mathematician's or the experimentalist's.\nAnd yet you won't be able to meet the usefulness test without\nimplying definite and fairly broadly applicable conclusions.  Worse\nstill, the usefulness test will tend to produce results that annoy\npeople: there's no use in telling people things they already believe,\nand people are often upset to be told things they don't.Here's the exciting thing, though.  Anyone can do this.  Getting\nto general plus useful by starting with useful and cranking up the\ngenerality may be unsuitable for junior professors trying to get\ntenure, but it's better for everyone else, including professors who\nalready have it.  This side of the mountain is a nice gradual slope.\nYou can start by writing things that are useful but very specific,\nand then gradually make them more general.  Joe's has good burritos.\nWhat makes a good burrito?  What makes good food?  What makes\nanything good?  You can take as long as you want.  You don't have\nto get all the way to the top of the mountain.  You don't have to\ntell anyone you're doing philosophy.If it seems like a daunting task to do philosophy, here's an\nencouraging thought.  The field is a lot younger than it seems.\nThough the first philosophers in the western tradition lived about\n2500 years ago, it would be misleading to say the field is 2500\nyears old, because for most of that time the leading practitioners\nweren't doing much more than writing commentaries on Plato or\nAristotle while watching over their shoulders for the next invading\narmy.  In the times when they weren't, philosophy was hopelessly\nintermingled with religion.  It didn't shake itself free till a\ncouple hundred years ago, and even then was afflicted by the\nstructural problems I've described above.  If I say this, some will\nsay it's a ridiculously overbroad and uncharitable generalization,\nand others will say it's old news, but here goes: judging from their\nworks, most philosophers up to the present have been wasting their\ntime.  So in a sense the field is still at the first step. \n[16]That sounds a preposterous claim to make.  It won't seem so\npreposterous in 10,000 years.  Civilization always seems old, because\nit's always the oldest it's ever been.  The only way to say whether\nsomething is really old or not is by looking at structural evidence,\nand structurally philosophy is young; it's still reeling from the\nunexpected breakdown of words.Philosophy is as young now as math was in 1500.  There is a lot\nmore to discover.Notes\n[1]\nIn practice formal logic is not much use, because despite\nsome progress in the last 150 years we're still only able to formalize\na small percentage of statements.  We may never do that much better,\nfor the same reason 1980s-style \"knowledge representation\" could\nnever have worked; many statements may have no representation more\nconcise than a huge, analog brain state.[2]\nIt was harder for Darwin's contemporaries to grasp this than\nwe can easily imagine.  The story of creation in the Bible is not\njust a Judeo-Christian concept; it's roughly what everyone must\nhave believed since before people were people.  The hard part of\ngrasping evolution was to realize that species weren't, as they\nseem to be, unchanging, but had instead evolved from different,\nsimpler organisms over unimaginably long periods of time.Now we don't have to make that leap.  No one in an industrialized\ncountry encounters the idea of evolution for the first time as an\nadult.  Everyone's taught about it as a child, either as truth or\nheresy.[3]\nGreek philosophers before Plato wrote in verse.  This must\nhave affected what they said.  If you try to write about the nature\nof the world in verse, it inevitably turns into incantation.  Prose\nlets you be more precise, and more tentative.[4]\nPhilosophy is like math's\nne'er-do-well brother.  It was born when Plato and Aristotle looked\nat the works of their predecessors and said in effect \"why can't\nyou be more like your brother?\"  Russell was still saying the same\nthing 2300 years later.Math is the precise half of the most abstract ideas, and philosophy\nthe imprecise half.  It's probably inevitable that philosophy will\nsuffer by comparison, because there's no lower bound to its precision.\nBad math is merely boring, whereas bad philosophy is nonsense.  And\nyet there are some good ideas in the imprecise half.[5]\nAristotle's best work was in logic and zoology, both of which\nhe can  be said to have invented.  But the most dramatic departure\nfrom his predecessors was a new, much more analytical style of\nthinking.  He was arguably the first scientist.[6]\nBrooks, Rodney, Programming in Common Lisp, Wiley, 1985, p.\n94.[7]\nSome would say we depend on Aristotle more than we realize,\nbecause his ideas were one of the ingredients in our common culture.\nCertainly a lot of the words we use have a connection with Aristotle,\nbut it seems a bit much to suggest that we wouldn't have the concept\nof the essence of something or the distinction between matter and\nform if Aristotle hadn't written about them.One way to see how much we really depend on Aristotle would be to\ndiff European culture with Chinese: what ideas did European culture\nhave in 1800 that Chinese culture didn't, in virtue of Aristotle's\ncontribution?[8]\nThe meaning of the word \"philosophy\" has changed over time.\nIn ancient times it covered a broad range of topics, comparable in\nscope to our \"scholarship\" (though without the methodological\nimplications).  Even as late as Newton's time it included what we\nnow call \"science.\"  But core of the subject today is still what\nseemed to Aristotle the core: the attempt to discover the most\ngeneral truths.Aristotle didn't call this \"metaphysics.\"  That name got assigned\nto it because the books we now call the Metaphysics came after\n(meta = after) the Physics in the standard edition of Aristotle's\nworks compiled by Andronicus of Rhodes three centuries later.  What\nwe call \"metaphysics\" Aristotle called \"first philosophy.\"[9]\nSome of Aristotle's immediate successors may have realized\nthis, but it's hard to say because most of their works are lost.[10]\nSokal, Alan, \"Transgressing the Boundaries: Toward a Transformative\nHermeneutics of Quantum Gravity,\" Social Text 46/47, pp. 217-252.Abstract-sounding nonsense seems to be most attractive when it's\naligned with some axe the audience already has to grind.  If this\nis so we should find it's most popular with groups that are (or\nfeel) weak.  The powerful don't need its reassurance.[11]\nLetter to Ottoline Morrell, December 1912.  Quoted in:Monk, Ray, Ludwig Wittgenstein: The Duty of Genius, Penguin, 1991,\np. 75.[12]\nA preliminary result, that all metaphysics between Aristotle\nand 1783 had been a waste of time, is due to I. Kant.[13]\nWittgenstein asserted a sort of mastery to which the inhabitants\nof early 20th century Cambridge seem to have been peculiarly\nvulnerable\u2014perhaps partly because so many had been raised religious\nand then stopped believing, so had a vacant space in their heads\nfor someone to tell them what to do (others chose Marx or Cardinal\nNewman), and partly because a quiet, earnest place like Cambridge\nin that era had no natural immunity to messianic figures, just as\nEuropean politics then had no natural immunity to dictators.[14]\nThis is actually from the Ordinatio of Duns Scotus (ca.\n1300), with \"number\" replaced by \"gender.\"  Plus ca change.Wolter, Allan (trans), Duns Scotus: Philosophical Writings, Nelson,\n1963, p. 92.[15]\nFrankfurt, Harry, On Bullshit,  Princeton University Press,\n2005.[16]\nSome introductions to philosophy now take the line that\nphilosophy is worth studying as a process rather than for any\nparticular truths you'll learn.  The philosophers whose works they\ncover would be rolling in their graves at that.  They hoped they\nwere doing more than serving as examples of how to argue: they hoped\nthey were getting results.  Most were wrong, but it doesn't seem\nan impossible hope.This argument seems to me like someone in 1500 looking at the lack\nof results achieved by alchemy and saying its value was as a process.\nNo, they were going about it wrong.  It turns out it is possible\nto transmute lead into gold (though not economically at current\nenergy prices), but the route to that knowledge was to\nbacktrack and try another approach.Thanks to Trevor Blackwell, Paul Buchheit, Jessica Livingston, \nRobert Morris, Mark Nitzberg, and Peter Norvig for reading drafts of this.November 2022Since I was about 9 I've been puzzled by the apparent contradiction\nbetween being made of matter that behaves in a predictable way, and\nthe feeling that I could choose to do whatever I wanted. At the\ntime I had a self-interested motive for exploring the question. At\nthat age (like most succeeding ages) I was always in trouble with\nthe authorities, and it seemed to me that there might possibly be\nsome way to get out of trouble by arguing that I wasn't responsible\nfor my actions. I gradually lost hope of that, but the puzzle\nremained: How do you reconcile being a machine made of matter with\nthe feeling that you're free to choose what you do?\n[1]The best way to explain the answer may be to start with a slightly\nwrong version, and then fix it. The wrong version is: You can do\nwhat you want, but you can't want what you want. Yes, you can control\nwhat you do, but you'll do what you want, and you can't control\nthat.The reason this is mistaken is that people do sometimes change what\nthey want. People who don't want to want something \u2014 drug addicts,\nfor example \u2014 can sometimes make themselves stop wanting it. And\npeople who want to want something \u2014 who want to like classical\nmusic, or broccoli \u2014 sometimes succeed.So we modify our initial statement: You can do what you want, but\nyou can't want to want what you want.That's still not quite true. It's possible to change what you want\nto want. I can imagine someone saying \"I decided to stop wanting\nto like classical music.\" But we're getting closer to the truth.\nIt's rare for people to change what they want to want, and the more\n\"want to\"s we add, the rarer it gets.We can get arbitrarily close to a true statement by adding more \"want\nto\"s in much the same way we can get arbitrarily close to 1 by adding\nmore 9s to a string of 9s following a decimal point. In practice\nthree or four \"want to\"s must surely be enough. It's hard even to\nenvision what it would mean to change what you want to want to want\nto want, let alone actually do it.So one way to express the correct answer is to use a regular\nexpression. You can do what you want, but there's some statement\nof the form \"you can't (want to)* want what you want\" that's true.\nUltimately you get back to a want that you don't control.\n[2]\nNotes[1]\nI didn't know when I was 9 that matter might behave randomly,\nbut I don't think it affects the problem much. Randomness destroys\nthe ghost in the machine as effectively as determinism.[2]\nIf you don't like using an expression, you can make the same\npoint using higher-order desires: There is some n such that you\ndon't control your nth-order desires.\nThanks to Trevor Blackwell,\nJessica Livingston, Robert Morris, and\nMichael Nielsen for reading drafts of this.December 2014If the world were static, we could have monotonically increasing\nconfidence in our beliefs.  The more (and more varied) experience\na belief survived, the less likely it would be false.  Most people\nimplicitly believe something like this about their opinions.  And\nthey're justified in doing so with opinions about things that don't\nchange much, like human nature.  But you can't trust your opinions\nin the same way about things that change, which could include\npractically everything else.When experts are wrong, it's often because they're experts on an\nearlier version of the world.Is it possible to avoid that?  Can you protect yourself against\nobsolete beliefs?  To some extent, yes. I spent almost a decade\ninvesting in early stage startups, and curiously enough protecting\nyourself against obsolete beliefs is exactly what you have to do\nto succeed as a startup investor.  Most really good startup ideas\nlook like bad ideas at first, and many of those look bad specifically\nbecause some change in the world just switched them from bad to\ngood.  I spent a lot of time learning to recognize such ideas, and\nthe techniques I used may be applicable to ideas in general.The first step is to have an explicit belief in change.  People who\nfall victim to a monotonically increasing confidence in their\nopinions are implicitly concluding the world is static.  If you\nconsciously remind yourself it isn't, you start to look for change.Where should one look for it?  Beyond the moderately useful\ngeneralization that human nature doesn't change much, the unfortunate\nfact is that change is hard to predict.  This is largely a tautology\nbut worth remembering all the same: change that matters usually\ncomes from an unforeseen quarter.So I don't even try to predict it.  When I get asked in interviews\nto predict the future, I always have to struggle to come up with\nsomething plausible-sounding on the fly, like a student who hasn't\nprepared for an exam.\n[1]\nBut it's not out of laziness that I haven't\nprepared.  It seems to me that beliefs about the future are so\nrarely correct that they usually aren't worth the extra rigidity\nthey impose, and that the best strategy is simply to be aggressively\nopen-minded.  Instead of trying to point yourself in the right\ndirection, admit you have no idea what the right direction is, and\ntry instead to be super sensitive to the winds of change.It's ok to have working hypotheses, even though they may constrain\nyou a bit, because they also motivate you.  It's exciting to chase\nthings and exciting to try to guess answers.  But you have to be\ndisciplined about not letting your hypotheses harden into anything\nmore.\n[2]I believe this passive m.o. works not just for evaluating new ideas\nbut also for having them.  The way to come up with new ideas is not\nto try explicitly to, but to try to solve problems and simply not\ndiscount weird hunches you have in the process.The winds of change originate in the unconscious minds of domain\nexperts.  If you're sufficiently expert in a field, any weird idea\nor apparently irrelevant question that occurs to you is ipso facto\nworth exploring. \n[3]\n Within Y Combinator, when an idea is described\nas crazy, it's a compliment\u2014in fact, on average probably a\nhigher compliment than when an idea is described as good.Startup investors have extraordinary incentives for correcting\nobsolete beliefs.  If they can realize before other investors that\nsome apparently unpromising startup isn't, they can make a huge\namount of money.  But the incentives are more than just financial.\nInvestors' opinions are explicitly tested: startups come to them\nand they have to say yes or no, and then, fairly quickly, they learn\nwhether they guessed right.  The investors who say no to a Google\n(and there were several) will remember it for the rest of their\nlives.Anyone who must in some sense bet on ideas rather than merely\ncommenting on them has similar incentives.  Which means anyone who\nwants such incentives can have them, by turning their comments into\nbets: if you write about a topic in some fairly durable and public\nform, you'll find you worry much more about getting things right\nthan most people would in a casual conversation.\n[4]Another trick I've found to protect myself against obsolete beliefs\nis to focus initially on people rather than ideas. Though the nature\nof future discoveries is hard to predict, I've found I can predict\nquite well what sort of people will make them.  Good new ideas come\nfrom earnest, energetic, independent-minded people.Betting on people over ideas saved me countless times as an investor.\nWe thought Airbnb was a bad idea, for example. But we could tell\nthe founders were earnest, energetic, and independent-minded.\n(Indeed, almost pathologically so.)  So we suspended disbelief and\nfunded them.This too seems a technique that should be generally applicable.\nSurround yourself with the sort of people new ideas come from.  If\nyou want to notice quickly when your beliefs become obsolete, you\ncan't do better than to be friends with the people whose discoveries\nwill make them so.It's hard enough already not to become the prisoner of your own\nexpertise, but it will only get harder, because change is accelerating.\nThat's not a recent trend; change has been accelerating since the\npaleolithic era.  Ideas beget ideas.  I don't expect that to change.\nBut I could be wrong.\nNotes[1]\nMy usual trick is to talk about aspects of the present that\nmost people haven't noticed yet.[2]\nEspecially if they become well enough known that people start\nto identify them with you.  You have to be extra skeptical about\nthings you want to believe, and once a hypothesis starts to be\nidentified with you, it will almost certainly start to be in that\ncategory.[3]\nIn practice \"sufficiently expert\" doesn't require one to be\nrecognized as an expert\u2014which is a trailing indicator in any\ncase.  In many fields a year of focused work plus caring a lot would\nbe enough.[4]\nThough they are public and persist indefinitely, comments on\ne.g. forums and places like Twitter seem empirically to work like\ncasual conversation.  The threshold may be whether what you write\nhas a title.\nThanks to Sam Altman, Patrick Collison, and Robert Morris\nfor reading drafts of this.April 2005\"Suits make a corporate comeback,\" says the New\nYork Times.  Why does this sound familiar?  Maybe because\nthe suit was also back in February,\n\nSeptember\n2004, June\n2004, March\n2004, September\n2003, \n\nNovember\n2002, \nApril 2002,\nand February\n2002.\n\nWhy do the media keep running stories saying suits are back?  Because\nPR firms tell \nthem to.  One of the most surprising things I discovered\nduring my brief business career was the existence of the PR industry,\nlurking like a huge, quiet submarine beneath the news.  Of the\nstories you read in traditional media that aren't about politics,\ncrimes, or disasters, more than half probably come from PR firms.I know because I spent years hunting such \"press hits.\"  Our startup spent\nits entire marketing budget on PR: at a time when we were assembling\nour own computers to save money, we were paying a PR firm $16,000\na month.  And they were worth it.  PR is the news equivalent of\nsearch engine optimization; instead of buying ads, which readers\nignore, you get yourself inserted directly into the stories.  [1]Our PR firm\nwas one of the best in the business.  In 18 months, they got press\nhits in over 60 different publications.  \nAnd we weren't the only ones they did great things for.  \nIn 1997 I got a call from another\nstartup founder considering hiring them to promote his company.  I\ntold him they were PR gods, worth every penny of their outrageous   \nfees.  But I remember thinking his company's name was odd.\nWhy call an auction site \"eBay\"?\nSymbiosisPR is not dishonest.  Not quite.  In fact, the reason the best PR\nfirms are so effective is precisely that they aren't dishonest.\nThey give reporters genuinely valuable information.  A good PR firm\nwon't bug reporters just because the client tells them to; they've\nworked hard to build their credibility with reporters, and they\ndon't want to destroy it by feeding them mere propaganda.If anyone is dishonest, it's the reporters.  The main reason PR  \nfirms exist is that reporters are lazy.  Or, to put it more nicely,\noverworked.  Really they ought to be out there digging up stories\nfor themselves.  But it's so tempting to sit in their offices and\nlet PR firms bring the stories to them.  After all, they know good\nPR firms won't lie to them.A good flatterer doesn't lie, but tells his victim selective truths\n(what a nice color your eyes are). Good PR firms use the same\nstrategy: they give reporters stories that are true, but whose truth\nfavors their clients.For example, our PR firm often pitched stories about how the Web  \nlet small merchants compete with big ones.  This was perfectly true.\nBut the reason reporters ended up writing stories about this\nparticular truth, rather than some other one, was that small merchants\nwere our target market, and we were paying the piper.Different publications vary greatly in their reliance on PR firms.\nAt the bottom of the heap are the trade press, who make most of\ntheir money from advertising and would give the magazines away for\nfree if advertisers would let them.  [2] The average\ntrade publication is a  bunch of ads, glued together by just enough\narticles to make it look like a magazine.  They're so desperate for\n\"content\" that some will print your press releases almost verbatim,\nif you take the trouble to write them to read like articles.At the other extreme are publications like the New York Times\nand the Wall Street Journal.  Their reporters do go out and\nfind their own stories, at least some of the time.  They'll listen \nto PR firms, but briefly and skeptically.  We managed to get press   \nhits in almost every publication we wanted, but we never managed \nto crack the print edition of the Times.  [3]The weak point of the top reporters is not laziness, but vanity.\nYou don't pitch stories to them.  You have to approach them as if\nyou were a specimen under their all-seeing microscope, and make it\nseem as if the story you want them to run is something they thought \nof themselves.Our greatest PR coup was a two-part one.  We estimated, based on\nsome fairly informal math, that there were about 5000 stores on the\nWeb.  We got one paper to print this number, which seemed neutral   \nenough.  But once this \"fact\" was out there in print, we could quote\nit to other publications, and claim that with 1000 users we had 20%\nof the online store market.This was roughly true.  We really did have the biggest share of the\nonline store market, and 5000 was our best guess at its size.  But\nthe way the story appeared in the press sounded a lot more definite.Reporters like definitive statements.  For example, many of the\nstories about Jeremy Jaynes's conviction say that he was one of the\n10 worst spammers.  This \"fact\" originated in Spamhaus's ROKSO list,\nwhich I think even Spamhaus would admit is a rough guess at the top\nspammers.  The first stories about Jaynes cited this source, but\nnow it's simply repeated as if it were part of the indictment.   \n[4]All you can say with certainty about Jaynes is that he was a fairly\nbig spammer.  But reporters don't want to print vague stuff like\n\"fairly big.\"  They want statements with punch, like \"top ten.\" And\nPR firms give them what they want.\nWearing suits, we're told, will make us \n3.6\npercent more productive.BuzzWhere the work of PR firms really does get deliberately misleading is in\nthe generation of \"buzz.\"  They usually feed the same story to    \nseveral different publications at once.  And when readers see similar\nstories in multiple places, they think there is some important trend\nafoot.  Which is exactly what they're supposed to think.When Windows 95 was launched, people waited outside stores\nat midnight to buy the first copies.  None of them would have been\nthere without PR firms, who generated such a buzz in\nthe news media that it became self-reinforcing, like a nuclear chain\nreaction.I doubt PR firms realize it yet, but the Web makes it possible to  \ntrack them at work.  If you search for the obvious phrases, you\nturn up several efforts over the years to place stories about the  \nreturn of the suit.  For example, the Reuters article \n\nthat got picked up by USA\nToday in September 2004.  \"The suit is back,\" it begins.Trend articles like this are almost always the work of\nPR firms.  Once you know how to read them, it's straightforward to\nfigure out who the client is.  With trend stories, PR firms usually\nline up one or more \"experts\" to talk about the industry generally. \nIn this case we get three: the NPD Group, the creative director of\nGQ, and a research director at Smith Barney.  [5] When\nyou get to the end of the experts, look for the client. And bingo, \nthere it is: The Men's Wearhouse.Not surprising, considering The Men's Wearhouse was at that moment \nrunning ads saying \"The Suit is Back.\"  Talk about a successful\npress hit-- a wire service article whose first sentence is your own\nad copy.The secret to finding other press hits from a given pitch\nis to realize that they all started from the same document back at\nthe PR firm.  Search for a few key phrases and the names of the\nclients and the experts, and you'll turn up other variants of this \nstory.Casual\nfridays are out and dress codes are in writes Diane E. Lewis\nin The Boston Globe.  In a remarkable coincidence, Ms. Lewis's\nindustry contacts also include the creative director of GQ.Ripped jeans and T-shirts are out, writes Mary Kathleen Flynn in\nUS News & World Report.  And she too knows the \ncreative director of GQ.Men's suits\nare back writes Nicole Ford in Sexbuzz.Com (\"the ultimate men's\nentertainment magazine\").Dressing\ndown loses appeal as men suit up at the office writes Tenisha\nMercer of The Detroit News.\nNow that so many news articles are online, I suspect you could find\na similar pattern for most trend stories placed by PR firms.  I\npropose we call this new sport \"PR diving,\" and I'm sure there are\nfar more striking examples out there than this clump of five stories.OnlineAfter spending years chasing them, it's now second nature\nto me to recognize press hits for what they are.  But before we\nhired a PR firm I had no idea where articles in the mainstream media\ncame from.  I could tell a lot of them were crap, but I didn't\nrealize why.Remember the exercises in critical reading you did in school, where\nyou had to look at a piece of writing and step back and ask whether\nthe author was telling the whole truth?  If you really want to be\na critical reader, it turns out you have to step back one step\nfurther, and ask not just whether the author is telling the truth,\nbut why he's writing about this subject at all.Online, the answer tends to be a lot simpler.  Most people who\npublish online write what they write for the simple reason that\nthey want to.  You\ncan't see the fingerprints of PR firms all over the articles, as\nyou can in so many print publications-- which is one of the reasons,\nthough they may not consciously realize it, that readers trust\nbloggers more than Business Week.I was talking recently to a friend who works for a\nbig newspaper.  He thought the print media were in serious trouble,\nand that they were still mostly in denial about it.  \"They think\nthe decline is cyclic,\" he said.  \"Actually it's structural.\"In other words, the readers are leaving, and they're not coming\nback.\nWhy? I think the main reason is that the writing online is more honest.\nImagine how incongruous the New York Times article about\nsuits would sound if you read it in a blog:\n   The urge to look corporate-- sleek, commanding,\n  prudent, yet with just a touch of hubris on your well-cut sleeve--\n  is an unexpected development in a time of business disgrace.\n   \nThe problem\nwith this article is not just that it originated in a PR firm.\nThe whole tone is bogus.  This is the tone of someone writing down\nto their audience.Whatever its flaws, the writing you find online\nis authentic.  It's not mystery meat cooked up\nout of scraps of pitch letters and press releases, and pressed into \nmolds of zippy\njournalese.  It's people writing what they think.I didn't realize, till there was an alternative, just how artificial\nmost of the writing in the mainstream media was.  I'm not saying\nI used to believe what I read in Time and Newsweek.  Since high\nschool, at least, I've thought of magazines like that more as\nguides to what ordinary people were being\ntold to think than as  \nsources of information.  But I didn't realize till the last  \nfew years that writing for publication didn't have to mean writing\nthat way.  I didn't realize you could write as candidly and\ninformally as you would if you were writing to a friend.Readers aren't the only ones who've noticed the\nchange.  The PR industry has too.\nA hilarious article\non the site of the PR Society of America gets to the heart of the   \nmatter:\n   Bloggers are sensitive about becoming mouthpieces\n  for other organizations and companies, which is the reason they\n  began blogging in the first place.  \nPR people fear bloggers for the same reason readers\nlike them.  And that means there may be a struggle ahead.  As\nthis new kind of writing draws readers away from traditional media, we\nshould be prepared for whatever PR mutates into to compensate.  \nWhen I think   \nhow hard PR firms work to score press hits in the traditional   \nmedia, I can't imagine they'll work any less hard to feed stories\nto bloggers, if they can figure out how.\nNotes[1] PR has at least   \none beneficial feature: it favors small companies.  If PR didn't  \nwork, the only alternative would be to advertise, and only big\ncompanies can afford that.[2] Advertisers pay \nless for ads in free publications, because they assume readers \nignore something they get for free.  This is why so many trade\npublications nominally have a cover price and yet give away free\nsubscriptions with such abandon.[3] Different sections\nof the Times vary so much in their standards that they're\npractically different papers.  Whoever fed the style section reporter\nthis story about suits coming back would have been sent packing by\nthe regular news reporters.[4] The most striking\nexample I know of this type is the \"fact\" that the Internet worm   \nof 1988 infected 6000 computers. I was there when it was cooked up,\nand this was the recipe: someone guessed that there were about\n60,000 computers attached to the Internet, and that the worm might\nhave infected ten percent of them.Actually no one knows how many computers the worm infected, because\nthe remedy was to reboot them, and this destroyed all traces.  But\npeople like numbers.  And so this one is now replicated\nall over the Internet, like a little worm of its own.[5] Not all were\nnecessarily supplied by the PR firm. Reporters sometimes call a few\nadditional sources on their own, like someone adding a few fresh \nvegetables to a can of soup.\nThanks to Ingrid Basset, Trevor Blackwell, Sarah Harlin, Jessica \nLivingston, Jackie McDonough, Robert Morris, and Aaron Swartz (who\nalso found the PRSA article) for reading drafts of this.Correction: Earlier versions used a recent\nBusiness Week article mentioning del.icio.us as an example\nof a press hit, but Joshua Schachter tells me \nit was spontaneous.May 2004When people care enough about something to do it well, those who\ndo it best tend to be far better than everyone else.  There's a\nhuge gap between Leonardo and second-rate contemporaries like\nBorgognone.  You see the same gap between Raymond Chandler and the\naverage writer of detective novels.  A top-ranked professional chess\nplayer could play ten thousand games against an ordinary club player\nwithout losing once.Like chess or painting or writing novels, making money is a very\nspecialized skill.   But for some reason we treat this skill\ndifferently.  No one complains when a few people surpass all the\nrest at playing chess or writing novels, but when a few people make\nmore money than the rest, we get editorials saying this is wrong.Why?  The pattern of variation seems no different than for any other\nskill.  What causes people to react so strongly when the skill is\nmaking money?I think there are three reasons we treat making money as different:\nthe misleading model of wealth we learn as children; the disreputable\nway in which, till recently, most fortunes were accumulated; and\nthe worry that great variations in income are somehow bad for\nsociety.  As far as I can tell, the first is mistaken, the second\noutdated, and the third empirically false.  Could it be that, in a\nmodern democracy, variation in income is actually a sign of health?The Daddy Model of WealthWhen I was five I thought electricity was created by electric\nsockets.  I didn't realize there were power plants out there\ngenerating it.  Likewise, it doesn't occur to most kids that wealth\nis something that has to be generated.  It seems to be something\nthat flows from parents.Because of the circumstances in which they encounter it, children\ntend to misunderstand wealth.  They confuse it with money.  They\nthink that there is a fixed amount of it.  And they think of it as\nsomething that's distributed by authorities (and so should be\ndistributed equally), rather than something that has to be created\n(and might be created unequally).In fact, wealth is not money.  Money is just a convenient way of\ntrading one form of wealth for another.  Wealth is the underlying\nstuff\u2014the goods and services we buy.  When you travel to a\nrich or poor country, you don't have to look at people's bank\naccounts to tell which kind you're in.  You can see\nwealth\u2014in buildings and streets, in the clothes and the health\nof the people.Where does wealth come from?  People make it.  This was easier to\ngrasp when most people lived on farms, and made many of the things\nthey wanted with their own hands.  Then you could see in the house,\nthe herds, and the granary the wealth that each family created.  It\nwas obvious then too that the wealth of the world was not a fixed\nquantity that had to be shared out, like slices of a pie.  If you\nwanted more wealth, you could make it.This is just as true today, though few of us create wealth directly\nfor ourselves (except for a few vestigial domestic tasks).  Mostly\nwe create wealth for other people in exchange for money, which we\nthen trade for the forms of wealth we want. \n[1]Because kids are unable to create wealth, whatever they have has\nto be given to them.  And when wealth is something you're given,\nthen of course it seems that it should be distributed equally.\n[2]\nAs in most families it is.  The kids see to that.  \"Unfair,\" they\ncry, when one sibling gets more than another.In the real world, you can't keep living off your parents.  If you\nwant something, you either have to make it, or do something of\nequivalent value for someone else, in order to get them to give you\nenough money to buy it.  In the real world, wealth is (except for\na few specialists like thieves and speculators) something you have\nto create, not something that's distributed by Daddy.  And since\nthe ability and desire to create it vary from person to person,\nit's not made equally.You get paid by doing or making something people want, and those\nwho make more money are often simply better at doing what people\nwant.  Top actors make a lot more money than B-list actors.  The\nB-list actors might be almost as charismatic, but when people go\nto the theater and look at the list of movies playing, they want\nthat extra oomph that the big stars have.Doing what people want is not the only way to get money, of course.\nYou could also rob banks, or solicit bribes, or establish a monopoly.\nSuch tricks account for some variation in wealth, and indeed for\nsome of the biggest individual fortunes, but they are not the root\ncause of variation in income.  The root cause of variation in income,\nas Occam's Razor implies, is the same as the root cause of variation\nin every other human skill.In the United States, the CEO of a large public company makes about\n100 times as much as the average person. \n[3]\nBasketball players\nmake about 128 times as much, and baseball players 72 times as much.\nEditorials quote this kind of statistic with horror.  But I have\nno trouble imagining that one person could be 100 times as productive\nas another.  In ancient Rome the price of slaves varied by\na factor of 50 depending on their skills. \n[4]\nAnd that's without\nconsidering motivation, or the extra leverage in productivity that\nyou can get from modern technology.Editorials about athletes' or CEOs' salaries remind me of early\nChristian writers, arguing from first principles about whether the\nEarth was round, when they could just walk outside and check.\n[5]\nHow much someone's work is worth is not a policy question.  It's\nsomething the market already determines.\"Are they really worth 100 of us?\" editorialists ask.  Depends on\nwhat you mean by worth.  If you mean worth in the sense of what\npeople will pay for their skills, the answer is yes, apparently.A few CEOs' incomes reflect some kind of wrongdoing.  But are there\nnot others whose incomes really do reflect the wealth they generate?\nSteve Jobs saved a company that was in a terminal decline.  And not\nmerely in the way a turnaround specialist does, by cutting costs;\nhe had to decide what Apple's next products should be.  Few others\ncould have done it.  And regardless of the case with CEOs, it's\nhard to see how anyone could argue that the salaries of professional\nbasketball players don't reflect supply and demand.It may seem unlikely in principle that one individual could really\ngenerate so much more wealth than another.  The key to this mystery\nis to revisit that question, are they really worth 100 of us?\nWould a basketball team trade one of their players for 100\nrandom people?  What would Apple's next product look like if you\nreplaced Steve Jobs with a committee of 100 random people? \n[6]\nThese\nthings don't scale linearly.  Perhaps the CEO or the professional\nathlete has only ten times (whatever that means) the skill and\ndetermination of an ordinary person.  But it makes all the difference\nthat it's concentrated in one individual.When we say that one kind of work is overpaid and another underpaid,\nwhat are we really saying?  In a free market, prices are determined\nby what buyers want.  People like baseball more than  poetry, so\nbaseball players make more than poets.  To say that a certain kind\nof work is underpaid is thus identical with saying that people want\nthe wrong things.Well, of course people want the wrong things.  It seems odd to be\nsurprised by that.  And it seems even odder to say that it's\nunjust that certain kinds of work are underpaid. \n[7]\nThen\nyou're saying that it's unjust that people want the wrong things.\nIt's  lamentable that people prefer reality TV and corndogs to\nShakespeare and steamed vegetables, but unjust?  That seems like\nsaying that blue is heavy, or that up is circular.The appearance of the word \"unjust\" here is the unmistakable spectral\nsignature of the Daddy Model.  Why else would this idea occur in\nthis odd context?  Whereas if the speaker were still operating on\nthe Daddy Model, and saw wealth as something that flowed from a\ncommon source and had to be shared out, rather than something\ngenerated by doing what other people wanted, this is exactly what\nyou'd get on noticing that some people made much more than others.When we talk about \"unequal distribution of income,\" we should\nalso ask, where does that income come from?\n[8]\nWho made the wealth\nit represents?  Because to the extent that income varies simply\naccording to how much wealth people create, the distribution may\nbe unequal, but it's hardly unjust.Stealing ItThe second reason we tend to find great disparities of wealth\nalarming is that for most of human history the usual way to accumulate\na fortune was to steal it: in pastoral societies by cattle raiding;\nin agricultural societies by appropriating others' estates in times\nof war, and taxing them in times of peace.In conflicts, those on the winning side would receive the estates\nconfiscated from the losers.  In England in the 1060s, when William\nthe Conqueror distributed the estates of the defeated Anglo-Saxon\nnobles to his followers, the conflict was military.  By the 1530s,\nwhen Henry VIII distributed the estates of the monasteries to his\nfollowers, it was mostly political. \n[9]\nBut the principle was the\nsame.  Indeed, the same principle is at work now in Zimbabwe.In more organized societies, like China, the ruler and his officials\nused taxation instead of confiscation.  But here too we see the\nsame principle: the way to get rich was not to create wealth, but\nto serve a ruler powerful enough to appropriate it.This started to change in Europe with the rise of the middle class.\nNow we think of the middle class as people who are neither rich nor\npoor, but originally they were a distinct group.  In a feudal\nsociety, there are just two classes: a warrior aristocracy, and the\nserfs who work their estates.  The middle class were a new, third\ngroup who lived in towns and supported themselves by manufacturing\nand trade.Starting in the tenth and eleventh centuries, petty nobles and\nformer serfs banded together in towns that gradually became powerful\nenough to ignore the local feudal lords. \n[10]\nLike serfs, the middle\nclass made a living largely by creating wealth.  (In port cities\nlike Genoa and Pisa, they also engaged in piracy.) But unlike serfs\nthey had an incentive to create a lot of it.  Any wealth a serf\ncreated belonged to his master.  There was not much point in making\nmore than you could hide.  Whereas the independence of the townsmen\nallowed them to keep whatever wealth they created.Once it became possible to get rich by creating wealth, society as\na whole started to get richer very rapidly.  Nearly everything we\nhave was created by the middle class.  Indeed, the other two classes\nhave effectively disappeared in industrial societies, and their\nnames been given to either end of the middle class.  (In the original\nsense of the word, Bill Gates is middle class.)But it was not till the Industrial Revolution that wealth creation\ndefinitively replaced corruption as the best way to get rich.  In\nEngland, at least, corruption only became unfashionable (and in\nfact only started to be called \"corruption\") when there started to\nbe other, faster ways to get rich.Seventeenth-century England was much like the third world today,\nin that government office was a recognized route to wealth.  The\ngreat fortunes of that time still derived more from what we would\nnow call corruption than from commerce. \n[11]\nBy the nineteenth\ncentury that had changed.  There continued to be bribes, as there\nstill are everywhere, but politics had by then been left to men who\nwere driven more by vanity than greed.  Technology had made it\npossible to create wealth faster than you could steal it.  The\nprototypical rich man of the nineteenth century was not a courtier\nbut an industrialist.With the rise of the middle class, wealth stopped being a zero-sum\ngame.  Jobs and Wozniak didn't have to make us poor to make themselves\nrich.  Quite the opposite: they created things that made our lives\nmaterially richer.  They had to, or we wouldn't have paid for them.But since for most of the world's history the main route to wealth\nwas to steal it, we tend to be suspicious of rich people.  Idealistic\nundergraduates find their unconsciously preserved child's model of\nwealth confirmed by eminent writers of the past.  It is a case of\nthe mistaken meeting the outdated.\"Behind every great fortune, there is a crime,\" Balzac wrote.  Except\nhe didn't.  What he actually said was that a great fortune with no\napparent cause was probably due to a crime well enough executed\nthat it had been forgotten.  If we were talking about Europe in\n1000, or most of the third world today, the standard misquotation\nwould be spot on.  But Balzac lived in nineteenth-century France,\nwhere the Industrial Revolution was well advanced.  He knew you\ncould make a fortune without stealing it.  After all, he did himself,\nas a popular novelist.\n[12]Only a few countries (by no coincidence, the richest ones) have\nreached this stage.  In most, corruption still has the upper hand.\nIn most, the fastest way to get wealth is by stealing it.  And so\nwhen we see increasing differences in income in a rich country,\nthere is a tendency to worry that it's sliding back toward becoming\nanother Venezuela.  I think the opposite is happening. I think\nyou're seeing a country a full step ahead of Venezuela.The Lever of TechnologyWill technology increase the gap between rich and poor?  It will\ncertainly increase the gap between the productive and the unproductive.\nThat's the whole point of technology.   With a tractor an energetic\nfarmer could plow six times as much land in a day as he could with\na team of horses.  But only if he mastered a new kind of farming.I've seen the lever of technology grow visibly in my own time.  In\nhigh school I made money by mowing lawns and scooping ice cream at\nBaskin-Robbins.  This was the only kind of work available at the\ntime.  Now high school kids could write software or design web\nsites.  But only some of them will; the rest will still be scooping\nice cream.I remember very vividly when in 1985 improved technology made it\npossible for me to buy a computer of my own.  Within months I was\nusing it to make money as a freelance programmer.  A few years\nbefore, I couldn't have done this.  A few years before, there was\nno such thing as a freelance programmer.  But Apple created\nwealth, in the form of powerful, inexpensive computers, and programmers\nimmediately set to work using it to create more.As this example suggests, the rate at which technology increases\nour productive capacity is probably exponential, rather than linear.\nSo we should expect to see ever-increasing variation in individual\nproductivity as time goes on.   Will that increase the gap between\nrich and the poor?  Depends which gap you mean.Technology should increase the gap in income, but it seems to\ndecrease other gaps.  A hundred years ago, the rich led a different\nkind of life from ordinary people.  They lived in houses\nfull of servants, wore elaborately uncomfortable clothes, and\ntravelled about in carriages drawn by teams of horses which themselves\nrequired their own houses and servants.  Now, thanks to technology,\nthe rich live more like the average person.Cars are a good example of why.  It's possible to buy expensive,\nhandmade cars that cost hundreds of thousands of dollars.  But there\nis not much point.  Companies make more money by building a large\nnumber of ordinary cars than a small number of expensive ones.  So\na company making a mass-produced car can afford to spend a lot more\non its design.  If you buy a custom-made car, something will always\nbe breaking.  The only point of buying one now is to advertise that\nyou can.Or consider watches.  Fifty years ago, by spending a lot of money\non a watch you could get better performance.  When watches had\nmechanical movements, expensive watches kept better time.  Not any\nmore.  Since the invention of the quartz movement, an ordinary Timex\nis more accurate than a Patek Philippe costing hundreds of thousands\nof dollars.\n[13]\nIndeed, as with expensive cars, if you're determined\nto spend a lot of money on a watch, you have to put up with some\ninconvenience to do it: as well as keeping worse time, mechanical\nwatches have to be wound.The only thing technology can't cheapen is brand.  Which is precisely\nwhy we hear ever more about it.  Brand is the residue left as the\nsubstantive differences between rich and poor evaporate.  But what\nlabel you have on your stuff is a much smaller matter than having\nit versus not having it.  In 1900, if you kept a carriage, no one\nasked what year or brand it was.  If you had one, you were rich.\nAnd if you weren't rich, you took the omnibus or walked.  Now even\nthe poorest Americans drive cars, and it is only because we're so\nwell trained by advertising that we can even recognize the especially\nexpensive ones.\n[14]The same pattern has played out in industry after industry.  If\nthere is enough demand for something, technology will make it cheap\nenough to sell in large volumes, and the mass-produced versions\nwill be, if not better, at least more convenient.\n[15]\nAnd there\nis nothing the rich like more than convenience.  The rich people I\nknow drive the same cars, wear the same clothes, have the same kind\nof furniture, and eat the same foods as my other friends.  Their\nhouses are in different neighborhoods, or if in the same neighborhood\nare different sizes, but within them life is similar.  The houses\nare made using the same construction techniques and contain much\nthe same objects.  It's inconvenient to do something expensive and\ncustom.The rich spend their time more like everyone else too.  Bertie\nWooster seems long gone.  Now, most people who are rich enough not\nto work do anyway.  It's not just social pressure that makes them;\nidleness is lonely and demoralizing.Nor do we have the social distinctions there were a hundred years\nago.   The novels and etiquette manuals of that period read now\nlike descriptions of some strange tribal society.  \"With respect\nto the continuance of friendships...\" hints Mrs. Beeton's Book\nof Household Management (1880), \"it may be found necessary, in\nsome cases, for a mistress to relinquish, on assuming the responsibility\nof a household, many of those commenced in the earlier part of her\nlife.\" A woman who married a rich man was expected to drop friends\nwho didn't.  You'd seem a barbarian if you behaved that way today.\nYou'd also have a very boring life.  People still tend to segregate\nthemselves somewhat, but much more on the basis of education than\nwealth.\n[16]Materially and socially, technology seems to be decreasing the gap\nbetween the rich and the poor, not increasing it.  If Lenin walked\naround the offices of a company like Yahoo or Intel or Cisco, he'd\nthink communism had won.  Everyone would be wearing the same clothes,\nhave the same kind of office (or rather, cubicle) with the same\nfurnishings, and address one another by their first names instead\nof by honorifics.  Everything would seem exactly as he'd predicted,\nuntil he looked at their bank accounts.  Oops.Is it a problem if technology increases that gap?  It doesn't seem\nto be so far.  As it increases the gap in income, it seems to\ndecrease most other gaps.Alternative to an AxiomOne often hears a policy criticized on the grounds that it would\nincrease the income gap between rich and poor.  As if it were an\naxiom that this would be bad.  It might be true that increased\nvariation in income would be bad, but I don't see how we can say\nit's axiomatic.Indeed, it may even be false, in industrial democracies.  In a\nsociety of serfs and warlords, certainly, variation in income is a\nsign of an underlying problem.  But serfdom is not the only cause\nof variation in income.  A 747 pilot doesn't make 40 times as much\nas a checkout clerk because he is a warlord who somehow holds her\nin thrall.  His skills are simply much more valuable.I'd like to propose an alternative idea: that in a modern society,\nincreasing variation in income is a sign of health.  Technology\nseems to increase the variation in productivity at faster than\nlinear rates.  If we don't see corresponding variation in income,\nthere are three possible explanations: (a) that technical innovation\nhas stopped, (b) that the people who would create the most wealth\naren't doing it, or (c) that they aren't getting paid for it.I think we can safely say that (a) and (b) would be bad.  If you\ndisagree, try living for a year using only the resources available\nto the average Frankish nobleman in 800, and report back to us.\n(I'll be generous and not send you back to the stone age.)The only option, if you're going to have an increasingly prosperous\nsociety without increasing variation in income, seems to be (c),\nthat people will create a lot of wealth without being paid for it.\nThat Jobs and Wozniak, for example, will cheerfully work 20-hour\ndays to produce the Apple computer for a society that allows them,\nafter taxes, to keep just enough of their income to match what they\nwould have made working 9 to 5 at a big company.Will people create wealth if they can't get paid for it?  Only if\nit's fun.  People will write operating systems for free.  But they\nwon't install them, or take support calls, or train customers to\nuse them.  And at least 90% of the work that even the highest tech\ncompanies do is of this second, unedifying kind.All the unfun kinds of wealth creation slow dramatically in a society\nthat confiscates private fortunes.  We can confirm this empirically.\nSuppose you hear a strange noise that you think may be due to a\nnearby fan.  You turn the fan off, and the noise stops.  You turn\nthe fan back on, and the noise starts again.  Off, quiet.  On,\nnoise.  In the absence of other information, it would seem the noise\nis caused by the fan.At various times and places in history, whether you could accumulate\na fortune by creating wealth has been turned on and off.  Northern\nItaly in 800, off (warlords would steal it).  Northern Italy in\n1100, on.  Central France in 1100, off (still feudal).  England in\n1800, on.  England in 1974, off (98% tax on investment income).\nUnited States in 1974, on.  We've even had a twin study: West\nGermany, on;  East Germany, off.  In every case, the creation of\nwealth seems to appear and disappear like the noise of a fan as you\nswitch on and off the prospect of keeping it.There is some momentum involved.  It probably takes at least a\ngeneration to turn people into East Germans (luckily for England).\nBut if it were merely a fan we were studying, without all the extra\nbaggage that comes from the controversial topic of wealth, no one\nwould have any doubt that the fan was causing the noise.If you suppress variations in income, whether by stealing private\nfortunes, as feudal rulers used to do, or by taxing them away, as\nsome modern governments have done, the result always seems to be\nthe same.    Society as a whole ends up poorer.If I had a choice of living in a society where I was materially\nmuch better off than I am now, but was among the poorest, or in one\nwhere I was the richest, but much worse off than I am now, I'd take\nthe first option.  If I had children, it would arguably be immoral\nnot to.  It's absolute poverty you want to avoid, not relative\npoverty.  If, as the evidence so far implies, you have to have one\nor the other in your society, take relative poverty.You need rich people in your society not so much because in spending\ntheir money they create jobs, but because of what they have to do\nto get rich.  I'm not talking about the trickle-down effect\nhere.  I'm not saying that if you let Henry Ford get rich, he'll\nhire you as a waiter at his next party.  I'm saying that he'll make\nyou a tractor to replace your horse.Notes[1]\nPart of the reason this subject is so contentious is that some\nof those most vocal on the subject of wealth\u2014university\nstudents, heirs, professors, politicians, and journalists\u2014have\nthe least experience creating it.  (This phenomenon will be familiar\nto anyone who has overheard conversations about sports in a bar.)Students are mostly still on the parental dole, and have not stopped\nto think about where that money comes from.  Heirs will be on the\nparental dole for life.  Professors and politicians live within\nsocialist eddies of the economy, at one remove from the creation\nof wealth, and are paid a flat rate regardless of how hard they\nwork.  And journalists as part of their professional code segregate\nthemselves from the revenue-collecting half of the businesses they\nwork for (the ad sales department).  Many of these people never\ncome face to face with the fact that the money they receive represents\nwealth\u2014wealth that, except in the case of journalists, someone\nelse created earlier.  They live in a world in which income is\ndoled out by a central authority according to some abstract notion\nof fairness (or randomly, in the case of heirs), rather than given\nby other people in return for something they wanted, so it may seem\nto them unfair that things don't work the same in the rest of the\neconomy.(Some professors do create a great deal of wealth for\nsociety.  But the money they're paid isn't a quid pro quo.\nIt's more in the nature of an investment.)[2]\nWhen one reads about the origins of the Fabian Society, it\nsounds like something cooked up by the high-minded Edwardian\nchild-heroes of Edith Nesbit's The Wouldbegoods.[3]\nAccording to a study by the Corporate Library, the median total\ncompensation, including salary, bonus, stock grants, and the exercise\nof stock options, of S&P 500 CEOs in 2002 was $3.65 million.\nAccording to Sports Illustrated, the average NBA player's\nsalary during the 2002-03 season was $4.54 million, and the average\nmajor league baseball player's salary at the start of the 2003\nseason was $2.56 million.  According to the Bureau of Labor\nStatistics, the mean annual wage in the US in 2002 was $35,560.[4]\nIn the early empire the price of an ordinary adult slave seems\nto have been about 2,000 sestertii (e.g. Horace, Sat. ii.7.43).\nA servant girl cost 600 (Martial vi.66), while Columella (iii.3.8)\nsays that a skilled vine-dresser was worth 8,000.  A doctor, P.\nDecimus Eros Merula, paid 50,000 sestertii for his freedom (Dessau,\nInscriptiones 7812).  Seneca (Ep. xxvii.7) reports\nthat one Calvisius Sabinus paid 100,000 sestertii apiece for slaves\nlearned in the Greek classics.  Pliny (Hist. Nat. vii.39)\nsays that the highest price paid for a slave up to his time was\n700,000 sestertii, for the linguist (and presumably teacher) Daphnis,\nbut that this had since been exceeded by actors buying their own\nfreedom.Classical Athens saw a similar variation in prices.  An ordinary\nlaborer was worth about 125 to 150 drachmae.  Xenophon (Mem.\nii.5) mentions prices ranging from 50 to 6,000 drachmae (for the\nmanager of a silver mine).For more on the economics of ancient slavery see:Jones, A. H. M., \"Slavery in the Ancient World,\" Economic History\nReview, 2:9 (1956), 185-199, reprinted in Finley, M. I. (ed.),\nSlavery in Classical Antiquity, Heffer, 1964.[5]\nEratosthenes (276\u2014195 BC) used shadow lengths in different\ncities to estimate the Earth's circumference.  He was off by only\nabout 2%.[6]\nNo, and Windows, respectively.[7]\nOne of the biggest divergences between the Daddy Model and\nreality is the valuation of hard work.  In the Daddy Model, hard\nwork is in itself deserving.  In reality, wealth is measured by\nwhat one delivers, not how much effort it costs.  If I paint someone's\nhouse, the owner shouldn't pay me extra for doing it with a toothbrush.It will seem to someone still implicitly operating on the Daddy\nModel that it is unfair when someone works hard and doesn't get\npaid much.  To help clarify the matter, get rid of everyone else\nand put our worker on a desert island, hunting and gathering fruit.\nIf he's bad at it he'll work very hard and not end up with much\nfood.  Is this unfair?  Who is being unfair to him?[8]\nPart of the reason for the tenacity of the Daddy Model may be\nthe dual meaning of \"distribution.\" When economists talk about\n\"distribution of income,\" they mean statistical distribution.  But\nwhen you use the phrase frequently, you can't help associating it\nwith the other sense of the word (as in e.g. \"distribution of alms\"),\nand thereby subconsciously seeing wealth as something that flows\nfrom some central tap.  The word \"regressive\" as applied to tax\nrates has a similar effect, at least on me; how can anything\nregressive be good?[9]\n\"From the beginning of the reign Thomas Lord Roos was an assiduous\ncourtier of the young Henry VIII and was soon to reap the rewards.\nIn 1525 he was made a Knight of the Garter and given the Earldom\nof Rutland.  In the thirties his support of the breach with Rome,\nhis zeal in crushing the Pilgrimage of Grace, and his readiness to\nvote the death-penalty in the succession of spectacular treason\ntrials that punctuated Henry's erratic matrimonial progress made\nhim an obvious candidate for grants of monastic property.\"Stone, Lawrence, Family and Fortune: Studies in Aristocratic\nFinance in the Sixteenth and Seventeenth Centuries, Oxford\nUniversity Press, 1973, p. 166.[10]\nThere is archaeological evidence for large settlements earlier,\nbut it's hard to say what was happening in them.Hodges, Richard and David Whitehouse, Mohammed, Charlemagne and\nthe Origins of Europe, Cornell University Press, 1983.[11]\nWilliam Cecil and his son Robert were each in turn the most\npowerful minister of the crown, and both used their position to\namass fortunes among the largest of their times.  Robert in particular\ntook bribery to the point of treason.  \"As Secretary of State and\nthe leading advisor to King James on foreign policy, [he] was a\nspecial recipient of favour, being offered large bribes by the Dutch\nnot to make peace with Spain, and large bribes by Spain to make\npeace.\" (Stone, op. cit., p. 17.)[12]\nThough Balzac made a lot of money from writing, he was notoriously\nimprovident and was troubled by debts all his life.[13]\nA Timex will gain or lose about .5 seconds per day.  The most\naccurate mechanical watch, the Patek Philippe 10 Day Tourbillon,\nis rated at -1.5 to +2 seconds.  Its retail price is about $220,000.[14]\nIf asked to choose which was more expensive, a well-preserved\n1989 Lincoln Town Car ten-passenger limousine ($5,000) or a 2004\nMercedes S600 sedan ($122,000), the average Edwardian might well\nguess wrong.[15]\nTo say anything meaningful about income trends, you have to\ntalk about real income, or income as measured in what it can buy.\nBut the usual way of calculating real income ignores much of the\ngrowth in wealth over time, because it depends on a consumer price\nindex created by bolting end to end a series of numbers that are\nonly locally accurate, and that don't include the prices of new\ninventions until they become so common that their prices stabilize.So while we might think it was very much better to live in a world\nwith antibiotics or air travel or an electric power grid than\nwithout, real income statistics calculated in the usual way will\nprove to us that we are only slightly richer for having these things.Another approach would be to ask, if you were going back to the\nyear x in a time machine, how much would you have to spend on trade\ngoods to make your fortune?  For example, if you were going back\nto 1970 it would certainly be less than $500, because the processing\npower you can get for $500 today would have been worth at least\n$150 million in 1970.  The function goes asymptotic fairly quickly,\nbecause for times over a hundred years or so you could get all you\nneeded in present-day trash.  In 1800 an empty plastic drink bottle\nwith a screw top would have seemed a miracle of workmanship.[16]\nSome will say this amounts to the same thing, because the rich\nhave better opportunities for education.  That's a valid point.  It\nis still possible, to a degree, to buy your kids' way into top\ncolleges by sending them to private schools that in effect hack the\ncollege admissions process.According to a 2002 report by the National Center for Education\nStatistics, about 1.7% of American kids attend private, non-sectarian\nschools.  At Princeton, 36% of the class of 2007 came from such\nschools.  (Interestingly, the number at Harvard is significantly\nlower, about 28%.)  Obviously this is a huge loophole.  It does at\nleast seem to be closing, not widening.Perhaps the designers of admissions processes should take a lesson\nfrom the example of computer security, and instead of just assuming\nthat their system can't be hacked, measure the degree to which it\nis.January 2016Life is short, as everyone knows. When I was a kid I used to wonder\nabout this. Is life actually short, or are we really complaining\nabout its finiteness?  Would we be just as likely to feel life was\nshort if we lived 10 times as long?Since there didn't seem any way to answer this question, I stopped\nwondering about it.  Then I had kids.  That gave me a way to answer\nthe question, and the answer is that life actually is short.Having kids showed me how to convert a continuous quantity, time,\ninto discrete quantities. You only get 52 weekends with your 2 year\nold.  If Christmas-as-magic lasts from say ages 3 to 10, you only\nget to watch your child experience it 8 times.  And while it's\nimpossible to say what is a lot or a little of a continuous quantity\nlike time, 8 is not a lot of something.  If you had a handful of 8\npeanuts, or a shelf of 8 books to choose from, the quantity would\ndefinitely seem limited, no matter what your lifespan was.Ok, so life actually is short.  Does it make any difference to know\nthat?It has for me.  It means arguments of the form \"Life is too short\nfor x\" have great force.  It's not just a figure of speech to say\nthat life is too short for something.  It's not just a synonym for\nannoying.  If you find yourself thinking that life is too short for\nsomething, you should try to eliminate it if you can.When I ask myself what I've found life is too short for, the word\nthat pops into my head is \"bullshit.\" I realize that answer is\nsomewhat tautological.  It's almost the definition of bullshit that\nit's the stuff that life is too short for.  And yet bullshit does\nhave a distinctive character.  There's something fake about it.\nIt's the junk food of experience.\n[1]If you ask yourself what you spend your time on that's bullshit,\nyou probably already know the answer.  Unnecessary meetings, pointless\ndisputes, bureaucracy, posturing, dealing with other people's\nmistakes, traffic jams, addictive but unrewarding pastimes.There are two ways this kind of thing gets into your life: it's\neither forced on you, or it tricks you.  To some extent you have to\nput up with the bullshit forced on you by circumstances.  You need\nto make money, and making money consists mostly of errands.  Indeed,\nthe law of supply and demand insures that: the more rewarding some\nkind of work is, the cheaper people will do it.  It may be that\nless bullshit is forced on you than you think, though.  There has\nalways been a stream of people who opt out of the default grind and\ngo live somewhere where opportunities are fewer in the conventional\nsense, but life feels more authentic.  This could become more common.You can do it on a smaller scale without moving.  The amount of\ntime you have to spend on bullshit varies between employers.  Most\nlarge organizations (and many small ones) are steeped in it.  But\nif you consciously prioritize bullshit avoidance over other factors\nlike money and prestige, you can probably find employers that will\nwaste less of your time.If you're a freelancer or a small company, you can do this at the\nlevel of individual customers.  If you fire or avoid toxic customers,\nyou can decrease the amount of bullshit in your life by more than\nyou decrease your income.But while some amount of bullshit is inevitably forced on you, the\nbullshit that sneaks into your life by tricking you is no one's\nfault but your own.  And yet the bullshit you choose may be harder\nto eliminate than the bullshit that's forced on you.  Things that\nlure you into wasting your time have to be really good at\ntricking you.  An example that will be familiar to a lot of people\nis arguing online.  When someone\ncontradicts you, they're in a sense attacking you. Sometimes pretty\novertly.  Your instinct when attacked is to defend yourself.  But\nlike a lot of instincts, this one wasn't designed for the world we\nnow live in.  Counterintuitive as it feels, it's better most of\nthe time not to defend yourself.  Otherwise these people are literally\ntaking your life.\n[2]Arguing online is only incidentally addictive. There are more\ndangerous things than that. As I've written before, one byproduct\nof technical progress is that things we like tend to become more\naddictive.  Which means we will increasingly have to make a conscious\neffort to avoid addictions \u0097 to stand outside ourselves and ask \"is\nthis how I want to be spending my time?\"As well as avoiding bullshit, one should actively seek out things\nthat matter.  But different things matter to different people, and\nmost have to learn what matters to them.  A few are lucky and realize\nearly on that they love math or taking care of animals or writing,\nand then figure out a way to spend a lot of time doing it.  But\nmost people start out with a life that's a mix of things that\nmatter and things that don't, and only gradually learn to distinguish\nbetween them.For the young especially, much of this confusion is induced by the\nartificial situations they find themselves in. In middle school and\nhigh school, what the other kids think of you seems the most important\nthing in the world.  But when you ask adults what they got wrong\nat that age, nearly all say they cared too much what other kids\nthought of them.One heuristic for distinguishing stuff that matters is to ask\nyourself whether you'll care about it in the future.  Fake stuff\nthat matters usually has a sharp peak of seeming to matter.  That's\nhow it tricks you.  The area under the curve is small, but its shape\njabs into your consciousness like a pin.The things that matter aren't necessarily the ones people would\ncall \"important.\"  Having coffee with a friend matters.  You won't\nfeel later like that was a waste of time.One great thing about having small children is that they make you\nspend time on things that matter: them. They grab your sleeve as\nyou're staring at your phone and say \"will you play with me?\" And\nodds are that is in fact the bullshit-minimizing option.If life is short, we should expect its shortness to take us by\nsurprise. And that is just what tends to happen.  You take things\nfor granted, and then they're gone.  You think you can always write\nthat book, or climb that mountain, or whatever, and then you realize\nthe window has closed.  The saddest windows close when other people\ndie. Their lives are short too.  After my mother died, I wished I'd\nspent more time with her.  I lived as if she'd always be there.\nAnd in her typical quiet way she encouraged that illusion.  But an\nillusion it was. I think a lot of people make the same mistake I\ndid.The usual way to avoid being taken by surprise by something is to\nbe consciously aware of it.  Back when life was more precarious,\npeople used to be aware of death to a degree that would now seem a\nbit morbid.  I'm not sure why, but it doesn't seem the right answer\nto be constantly reminding oneself of the grim reaper hovering at\neveryone's shoulder.  Perhaps a better solution is to look at the\nproblem from the other end. Cultivate a habit of impatience about\nthe things you most want to do. Don't wait before climbing that\nmountain or writing that book or visiting your mother.  You don't\nneed to be constantly reminding yourself why you shouldn't wait.\nJust don't wait.I can think of two more things one does when one doesn't have much\nof something: try to get more of it, and savor what one has.  Both\nmake sense here.How you live affects how long you live.  Most people could do better.\nMe among them.But you can probably get even more effect by paying closer attention\nto the time you have.  It's easy to let the days rush by.  The\n\"flow\" that imaginative people love so much has a darker cousin\nthat prevents you from pausing to savor life amid the daily slurry\nof errands and alarms.  One of the most striking things I've read\nwas not in a book, but the title of one: James Salter's Burning\nthe Days.It is possible to slow time somewhat. I've gotten better at it.\nKids help.  When you have small children, there are a lot of moments\nso perfect that you can't help noticing.It does help too to feel that you've squeezed everything out of\nsome experience.  The reason I'm sad about my mother is not just\nthat I miss her but that I think of all the things we could have\ndone that we didn't.  My oldest son will be 7 soon.  And while I\nmiss the 3 year old version of him, I at least don't have any regrets\nover what might have been.  We had the best time a daddy and a 3\nyear old ever had.Relentlessly prune bullshit, don't wait to do things that matter,\nand savor the time you have.  That's what you do when life is short.Notes[1]\nAt first I didn't like it that the word that came to mind was\none that had other meanings.  But then I realized the other meanings\nare fairly closely related.  Bullshit in the sense of things you\nwaste your time on is a lot like intellectual bullshit.[2]\nI chose this example deliberately as a note to self.  I get\nattacked a lot online.  People tell the craziest lies about me.\nAnd I have so far done a pretty mediocre job of suppressing the\nnatural human inclination to say \"Hey, that's not true!\"Thanks to Jessica Livingston and Geoff Ralston for reading drafts\nof this.July 2006\nWhen I was in high school I spent a lot of time imitating bad\nwriters.  What we studied in English classes was mostly fiction,\nso I assumed that was the highest form of writing.  Mistake number\none.  The stories that seemed to be most admired were ones in which\npeople suffered in complicated ways.  Anything funny or\ngripping was ipso facto suspect, unless it was old enough to be hard to\nunderstand, like Shakespeare or Chaucer.  Mistake number two.  The\nideal medium seemed the short story, which I've since learned had\nquite a brief life, roughly coincident with the peak of magazine\npublishing.  But since their size made them perfect for use in\nhigh school classes, we read a lot of them, which gave us the\nimpression the short story was flourishing.  Mistake number three.\nAnd because they were so short, nothing really had to happen; you\ncould just show a randomly truncated slice of life, and that was\nconsidered advanced.  Mistake number four.  The result was that I\nwrote a lot of stories in which nothing happened except that someone\nwas unhappy in a way that seemed deep.For most of college I was a philosophy major.  I was very impressed\nby the papers published in philosophy journals.  They were so\nbeautifully typeset, and their tone was just captivating\u2014alternately\ncasual and buffer-overflowingly technical.  A fellow would be walking\nalong a street and suddenly modality qua modality would spring upon\nhim.  I didn't ever quite understand these papers, but I figured\nI'd get around to that later, when I had time to reread them more\nclosely.  In the meantime I tried my best to imitate them.  This\nwas, I can now see, a doomed undertaking, because they weren't\nreally saying anything.  No philosopher ever refuted another, for\nexample, because no one said anything definite enough to refute.\nNeedless to say, my imitations didn't say anything either.In grad school I was still wasting time imitating the wrong things.\nThere was then a fashionable type of program called an expert system,\nat the core of which was something called an inference engine.  I\nlooked at what these things did and thought \"I could write that in\na thousand lines of code.\"  And yet eminent professors were writing\nbooks about them, and startups were selling them for a year's salary\na copy.  What an opportunity, I thought; these impressive things\nseem easy to me; I must be pretty sharp.  Wrong.  It was simply a\nfad.  The books the professors wrote about expert systems are now\nignored.  They were not even on a path to anything interesting.\nAnd the customers paying so much for them were largely the same\ngovernment agencies that paid thousands for screwdrivers and toilet\nseats.How do you avoid copying the wrong things?  Copy only what you\ngenuinely like.  That would have saved me in all three cases.  I\ndidn't enjoy the short stories we had to read in English classes;\nI didn't learn anything from philosophy papers; I didn't use expert\nsystems myself.  I believed these things were good because they\nwere admired.It can be hard to separate the things you like from the things\nyou're impressed with.  One trick is to ignore presentation.  Whenever\nI see a painting impressively hung in a museum, I ask myself: how\nmuch would I pay for this if I found it at a garage sale, dirty and\nframeless, and with no idea who painted it?  If you walk around a\nmuseum trying this experiment, you'll find you get some truly\nstartling results.  Don't ignore this data point just because it's\nan outlier.Another way to figure out what you like is to look at what you enjoy\nas guilty pleasures.  Many things people like, especially if they're\nyoung and ambitious, they like largely for the feeling of virtue\nin liking them.  99% of people reading Ulysses are thinking\n\"I'm reading Ulysses\" as they do it. A guilty pleasure is\nat least a pure one.  What do you read when you don't feel up to being\nvirtuous?  What kind of book do you read and feel sad that there's\nonly half of it left, instead of being impressed that you're half\nway through?  That's what you really like.Even when you find genuinely good things to copy, there's another\npitfall to be avoided.  Be careful to copy what makes them good,\nrather than their flaws.  It's easy to be drawn into imitating\nflaws, because they're easier to see, and of course easier to copy\ntoo.  For example, most painters in the eighteenth and nineteenth\ncenturies used brownish colors.  They were imitating the great\npainters of the Renaissance, whose paintings by that time were brown\nwith dirt.  Those paintings have since been cleaned, revealing\nbrilliant colors; their imitators are of course still brown.It was painting, incidentally, that cured me of copying the wrong\nthings.  Halfway through grad school I decided I wanted to try being\na painter, and the art world was so manifestly corrupt that it\nsnapped the leash of credulity.  These people made philosophy\nprofessors seem as scrupulous as mathematicians.  It was so clearly\na choice of doing good work xor being an insider that I was forced\nto see the distinction.  It's there to some degree in almost every\nfield, but I had till then managed to avoid facing it.That was one of the most valuable things I learned from painting:\nyou have to figure out for yourself what's \ngood.  You can't trust\nauthorities. They'll lie to you on this one.\n\nComment on this essay.May 2003If Lisp is so great, why don't more people use it?  I was    \nasked this question by a student in the audience at a \ntalk I gave recently.  Not for the first time, either.In languages, as in so many things, there's not much     \ncorrelation between popularity and quality.  Why does   \nJohn Grisham (King of Torts sales rank, 44) outsell\nJane Austen (Pride and Prejudice sales rank, 6191)?\nWould even Grisham claim that it's because he's a better\nwriter?Here's the first sentence of Pride and Prejudice:\n\nIt is a truth universally acknowledged, that a single man \nin possession of a good fortune must be in want of a\nwife.\n\n\"It is a truth universally acknowledged?\"  Long words for\nthe first sentence of a love story.Like Jane Austen, Lisp looks hard.  Its syntax, or lack\nof syntax, makes it look completely unlike \nthe languages\nmost people are used to.  Before I learned Lisp, I was afraid\nof it too.  I recently came across a notebook from 1983\nin which I'd written:\n\nI suppose I should learn Lisp, but it seems so foreign.\n\nFortunately, I was 19 at the time and not too resistant to learning\nnew things.  I was so ignorant that learning\nalmost anything meant learning new things.People frightened by Lisp make up other reasons for not\nusing it.  The standard\nexcuse, back when C was the default language, was that Lisp\nwas too slow.  Now that Lisp dialects are among\nthe faster\nlanguages available, that excuse has gone away.\nNow the standard excuse is openly circular: that other languages\nare more popular.(Beware of such reasoning.  It gets you Windows.)Popularity is always self-perpetuating, but it's especially\nso in programming languages. More libraries\nget written for popular languages, which makes them still\nmore popular.  Programs often have to work with existing programs,\nand this is easier if they're written in the same language,\nso languages spread from program to program like a virus.\nAnd managers prefer popular languages, because they give them \nmore leverage over developers, who can more easily be replaced.Indeed, if programming languages were all more or less equivalent,\nthere would be little justification for using any but the most\npopular.  But they aren't all equivalent, not by a long\nshot.  And that's why less popular languages, like Jane Austen's \nnovels, continue to survive at all.  When everyone else is reading \nthe latest John Grisham novel, there will always be a few people \nreading Jane Austen instead.May 2021Noora Health, a nonprofit I've \nsupported for years, just launched\na new NFT. It has a dramatic name, Save Thousands of Lives,\nbecause that's what the proceeds will do.Noora has been saving lives for 7 years. They run programs in\nhospitals in South Asia to teach new mothers how to take care of\ntheir babies once they get home. They're in 165 hospitals now. And\nbecause they know the numbers before and after they start at a new\nhospital, they can measure the impact they have. It is massive.\nFor every 1000 live births, they save 9 babies.This number comes from a study\nof 133,733 families at 28 different\nhospitals that Noora conducted in collaboration with the Better\nBirth team at Ariadne Labs, a joint center for health systems\ninnovation at Brigham and Women\u0092s Hospital and Harvard T.H. Chan\nSchool of Public Health.Noora is so effective that even if you measure their costs in the\nmost conservative way, by dividing their entire budget by the number\nof lives saved, the cost of saving a life is the lowest I've seen.\n$1,235.For this NFT, they're going to issue a public report tracking how\nthis specific tranche of money is spent, and estimating the number\nof lives saved as a result.NFTs are a new territory, and this way of using them is especially\nnew, but I'm excited about its potential. And I'm excited to see\nwhat happens with this particular auction, because unlike an NFT\nrepresenting something that has already happened,\nthis NFT gets better as the price gets higher.The reserve price was about $2.5 million, because that's what it\ntakes for the name to be accurate: that's what it costs to save\n2000 lives. But the higher the price of this NFT goes, the more\nlives will be saved. What a sentence to be able to write.August 2021When people say that in their experience all programming languages\nare basically equivalent, they're making a statement not about\nlanguages but about the kind of programming they've done.99.5% of programming consists of gluing together calls to library\nfunctions. All popular languages are equally good at this. So one\ncan easily spend one's whole career operating in the intersection\nof popular programming languages.But the other .5% of programming is disproportionately interesting.\nIf you want to learn what it consists of, the weirdness of weird\nlanguages is a good clue to follow.Weird languages aren't weird by accident. Not the good ones, at\nleast. The weirdness of the good ones usually implies the existence\nof some form of programming that's not just the usual gluing together\nof library calls.A concrete example: Lisp macros. Lisp macros seem weird even to\nmany Lisp programmers. They're not only not in the intersection of\npopular languages, but by their nature would be hard to implement\nproperly in a language without turning it into a dialect of\nLisp. And macros are definitely evidence of techniques that go\nbeyond glue programming. For example, solving problems by first\nwriting a language for problems of that type, and then writing\nyour specific application in it. Nor is this all you can do with\nmacros; it's just one region in a space of program-manipulating\ntechniques that even now is far from fully explored.So if you want to expand your concept of what programming can be,\none way to do it is by learning weird languages. Pick a language\nthat most programmers consider weird but whose median user is smart,\nand then focus on the differences between this language and the\nintersection of popular languages. What can you say in this language\nthat would be impossibly inconvenient to say in others? In the\nprocess of learning how to say things you couldn't previously say,\nyou'll probably be learning how to think things you couldn't\npreviously think.\nThanks to Trevor Blackwell, Patrick Collison, Daniel Gackle, Amjad\nMasad, and Robert Morris for reading drafts of this.\n\n\nWant to start a startup?  Get funded by\nY Combinator.\n\n\n\n\nOctober 2010After barely changing at all for decades, the startup funding\nbusiness is now in what could, at least by comparison, be called\nturmoil.  At Y Combinator we've seen dramatic changes in the funding\nenvironment for startups.  Fortunately one of them is much higher\nvaluations.The trends we've been seeing are probably not YC-specific.  I wish\nI could say they were, but the main cause is probably just that we\nsee trends first\u2014partly because the startups we fund are very\nplugged into the Valley and are quick to take advantage of anything\nnew, and partly because we fund so many that we have enough data\npoints to see patterns clearly.What we're seeing now, everyone's probably going to be seeing in\nthe next couple years.  So I'm going to explain what we're seeing,\nand what that will mean for you if you try to raise money.Super-AngelsLet me start by describing what the world of startup funding used\nto look like.  There used to be two sharply differentiated types\nof investors: angels and venture capitalists.  Angels are individual\nrich people who invest small amounts of their own money, while VCs\nare employees of funds that invest large amounts of other people's.For decades there were just those two types of investors, but now\na third type has appeared halfway between them: the so-called\nsuper-angels. \n[1]\n  And VCs have been provoked by their arrival\ninto making a lot of angel-style investments themselves.  So the\npreviously sharp line between angels and VCs has become hopelessly\nblurred.There used to be a no man's land between angels and VCs.  Angels\nwould invest $20k to $50k apiece, and VCs usually a million or more.\nSo an angel round meant a collection of angel investments that\ncombined to maybe $200k, and a VC round meant a series A round in\nwhich a single VC fund (or occasionally two) invested $1-5 million.The no man's land between angels and VCs was a very inconvenient\none for startups, because it coincided with the amount many wanted\nto raise.  Most startups coming out of Demo Day wanted to raise\naround $400k.  But it was a pain to stitch together that much out\nof angel investments, and most VCs weren't interested in investments\nso small.  That's the fundamental reason the super-angels have\nappeared.  They're responding to the market.The arrival of a new type of investor is big news for startups,\nbecause there used to be only two and they rarely competed with one\nanother.  Super-angels compete with both angels and VCs.  That's\ngoing to change the rules about how to raise money.  I don't know\nyet what the new rules will be, but it looks like most of the changes\nwill be for the better.A super-angel has some of the qualities of an angel, and some of\nthe qualities of a VC.  They're usually individuals, like angels.\nIn fact many of the current super-angels were initially angels of\nthe classic type.  But like VCs, they invest other people's money.\nThis allows them to invest larger amounts than angels:  a typical\nsuper-angel investment is currently about $100k.  They make investment\ndecisions quickly, like angels.  And they make a lot more investments\nper partner than VCs\u2014up to 10 times as many.The fact that super-angels invest other people's money makes them\ndoubly alarming to VCs. They don't just compete for startups; they\nalso compete for investors.  What super-angels really are is a new\nform of fast-moving, lightweight VC fund.   And those of us in the\ntechnology world know what usually happens when something comes\nalong that can be described in terms like that.  Usually it's the\nreplacement.Will it be?  As of now, few of the startups that take money from\nsuper-angels are ruling out taking VC money.  They're just postponing\nit.  But that's still a problem for VCs.  Some of the startups that\npostpone raising VC money may do so well on the angel money they\nraise that they never bother to raise more.  And those who do raise\nVC rounds will be able to get higher valuations when they do.  If\nthe best startups get 10x higher valuations when they raise series\nA rounds, that would cut VCs' returns from winners at least tenfold.\n[2]So I think VC funds are seriously threatened by the super-angels.\nBut one thing that may save them to some extent is the uneven\ndistribution of startup outcomes: practically all the returns are\nconcentrated in a few big successes.  The expected value of a startup\nis the percentage chance it's Google.  So to the extent that winning\nis a matter of absolute returns, the super-angels could win practically\nall the battles for individual startups and yet lose the war, if\nthey merely failed to get those few big winners.  And there's a\nchance that could happen, because the top VC funds have better\nbrands, and can also do more for their portfolio companies.  \n[3]Because super-angels make more investments per partner, they have\nless partner per investment.  They can't pay as much attention to\nyou as a VC on your board could.  How much is that extra attention\nworth?  It will vary enormously from one partner to another.  There's\nno consensus yet in the general case.  So for now this is something\nstartups are deciding individually.Till now, VCs' claims about how much value they added were sort of\nlike the government's.  Maybe they made you feel better, but you\nhad no choice in the matter, if you needed money on the scale only\nVCs could supply.  Now that VCs have competitors, that's going to\nput a market price on the help they offer.  The interesting thing\nis, no one knows yet what it will be.Do startups that want to get really big need the sort of advice and\nconnections only the top VCs can supply?  Or would super-angel money\ndo just as well?  The VCs will say you need them, and the super-angels\nwill say you don't.  But the truth is, no one knows yet, not even\nthe VCs and super-angels themselves.   All the super-angels know\nis that their new model seems promising enough to be worth trying,\nand all the VCs know is that it seems promising enough to worry\nabout.RoundsWhatever the outcome, the conflict between VCs and super-angels is\ngood news for founders.  And not just for the obvious reason that\nmore competition for deals means better terms.  The whole shape of\ndeals is changing.One of the biggest differences between angels and VCs is the amount\nof your company they want.  VCs want a lot.  In a series A round\nthey want a third of your company, if they can get it.  They don't\ncare much how much they pay for it, but they want a lot because the\nnumber of series A investments they can do is so small.  In a\ntraditional series A investment, at least one partner from the VC\nfund takes a seat on your board.  \n[4]\n Since board seats last about\n5 years and each partner can't handle more than about 10 at once,\nthat means a VC fund can only do about 2 series A deals per partner\nper year. And that means they need to get as much of the company\nas they can in each one.  You'd have to be a very promising startup\nindeed to get a VC to use up one of his 10 board seats for only a\nfew percent of you.Since angels generally don't take board seats, they don't have this\nconstraint.  They're happy to buy only a few percent of you.  And\nalthough the super-angels are in most respects mini VC funds, they've\nretained this critical property of angels.  They don't take board\nseats, so they don't need a big percentage of your company.Though that means you'll get correspondingly less attention from\nthem, it's good news in other respects.  Founders never really liked\ngiving up as much equity as VCs wanted.  It was a lot of the company\nto give up in one shot.  Most founders doing series A deals would\nprefer to take half as much money for half as much stock, and then\nsee what valuation they could get for the second half of the stock\nafter using the first half of the money to increase its value.  But\nVCs never offered that option.Now startups have another alternative.  Now it's easy to raise angel\nrounds about half the size of series A rounds.  Many of the startups\nwe fund are taking this route, and I predict that will be true of\nstartups in general.A typical big angel round might be $600k on a convertible note with\na valuation cap of $4 million premoney.  Meaning that when the note\nconverts into stock (in a later round, or upon acquisition), the\ninvestors in that round will get .6 / 4.6, or 13% of the company.\nThat's a lot less than the 30 to 40% of the company you usually\ngive up in a series A round if you do it so early.  \n[5]But the advantage of these medium-sized rounds is not just that\nthey cause less dilution.  You also lose less control.  After an\nangel round, the founders almost always still have control of the\ncompany, whereas after a series A round they often don't.  The\ntraditional board structure after a series A round is two founders,\ntwo VCs, and a (supposedly) neutral fifth person.  Plus series A\nterms usually give the investors a veto over various kinds of\nimportant decisions, including selling the company.  Founders usually\nhave a lot of de facto control after a series A, as long as things\nare going well.  But that's not the same as just being able to do\nwhat you want, like you could before.A third and quite significant advantage of angel rounds is that\nthey're less stressful to raise.  Raising a traditional series A\nround has in the past taken weeks, if not months.  When a VC firm\ncan only do 2 deals per partner per year, they're careful about\nwhich they do.  To get a traditional series A round you have to go\nthrough a series of meetings, culminating in a full partner meeting\nwhere the firm as a whole says yes or no.  That's the really scary\npart for founders: not just that series A rounds take so long, but\nat the end of this long process the VCs might still say no.  The\nchance of getting rejected after the full partner meeting averages\nabout 25%.  At some firms it's over 50%.Fortunately for founders, VCs have been getting a lot faster.\nNowadays Valley VCs are more likely to take 2 weeks than 2 months.\nBut they're still not as fast as angels and super-angels, the most\ndecisive of whom sometimes decide in hours.Raising an angel round is not only quicker, but you get feedback\nas it progresses.  An angel round is not an all or nothing thing\nlike a series A.  It's composed of multiple investors with varying\ndegrees of seriousness, ranging from the upstanding ones who commit\nunequivocally to the jerks who give you lines like \"come back to\nme to fill out the round.\" You usually start collecting money from\nthe most committed investors and work your way out toward the\nambivalent ones, whose interest increases as the round fills up.But at each point you know how you're doing.  If investors turn\ncold you may have to raise less, but when investors in an angel\nround turn cold the process at least degrades gracefully, instead\nof blowing up in your face and leaving you with nothing, as happens\nif you get rejected by a VC fund after a full partner meeting.\nWhereas if investors seem hot, you can not only close the round\nfaster, but now that convertible notes are becoming the norm,\nactually raise the price to reflect demand.ValuationHowever, the VCs have a weapon they can use against the super-angels,\nand they have started to use it.   VCs have started making angel-sized\ninvestments too.  The term \"angel round\" doesn't mean that all the\ninvestors in it are angels; it just describes the structure of the\nround.  Increasingly the participants include VCs making investments\nof a hundred thousand or two.  And when VCs invest in angel rounds\nthey can do things that super-angels don't like.  VCs are quite\nvaluation-insensitive in angel rounds\u2014partly because they are\nin general, and partly because they don't care that much about the\nreturns on angel rounds, which they still view mostly as a way to\nrecruit startups for series A rounds later.  So VCs who invest in\nangel rounds can blow up the valuations for angels and super-angels\nwho invest in them. \n[6]Some super-angels seem to care about valuations.  Several turned\ndown YC-funded startups after Demo Day because their valuations\nwere too high.  This was not a problem for the startups; by definition\na high valuation means enough investors were willing to accept it.\nBut it was mysterious to me that the super-angels would quibble\nabout valuations.  Did they not understand that the big returns\ncome from a few big successes, and that it therefore mattered far\nmore which startups you picked than how much you paid for them?After thinking about it for a while and observing certain other\nsigns, I have a theory that explains why the super-angels may be\nsmarter than they seem.  It would make sense for super-angels to\nwant low valuations if they're hoping to invest in startups that\nget bought early.  If you're hoping to hit the next Google, you\nshouldn't care if the valuation is 20 million.  But if you're looking\nfor companies that are going to get bought for 30 million, you care.\nIf you invest at 20 and the company gets bought for 30, you only\nget 1.5x.  You might as well buy Apple.So if some of the super-angels were looking for companies that could\nget acquired quickly, that would explain why they'd care about\nvaluations.  But why would they be looking for those?   Because\ndepending on the meaning of \"quickly,\" it could actually be very\nprofitable.  A company that gets acquired for 30 million is a failure\nto a VC, but it could be a 10x return for an angel, and moreover,\na quick 10x return.  Rate of return is what matters in\ninvesting\u2014not the multiple you get, but the multiple per year.\nIf a super-angel gets 10x in one year, that's a higher rate of\nreturn than a VC could ever hope to get from a company that took 6\nyears to go public.  To get the same rate of return, the VC would\nhave to get a multiple of 10^6\u2014one million x.  Even Google\ndidn't come close to that.So I think at least some super-angels are looking for companies\nthat will get bought.  That's the only rational explanation for\nfocusing on getting the right valuations, instead of the right\ncompanies.  And if so they'll be different to deal with than VCs.\nThey'll be tougher on valuations, but more accommodating if you want\nto sell early.PrognosisWho will win, the super-angels or the VCs?  I think the answer to\nthat is, some of each.  They'll each become more like one another.\nThe super-angels will start to invest larger amounts, and the VCs\nwill gradually figure out ways to make more, smaller investments\nfaster.  A decade from now the players will be hard to tell apart,\nand there will probably be survivors from each group.What does that mean for founders?  One thing it means is that the\nhigh valuations startups are presently getting may not last forever.\nTo the extent that valuations are being driven up by price-insensitive\nVCs, they'll fall again if VCs become more like super-angels and\nstart to become more miserly about valuations.  Fortunately if this\ndoes happen it will take years.The short term forecast is more competition between investors, which\nis good news for you.  The super-angels will try to undermine the\nVCs by acting faster, and the VCs will try to undermine the\nsuper-angels by driving up valuations.  Which for founders will\nresult in the perfect combination: funding rounds that close fast,\nwith high valuations.But remember that to get that combination, your startup will have\nto appeal to both super-angels and VCs.  If you don't seem like you\nhave the potential to go public, you won't be able to use VCs to\ndrive up the valuation of an angel round.There is a danger of having VCs in an angel round: the so-called\nsignalling risk.  If VCs are only doing it in the hope of investing\nmore later, what happens if they don't?  That's a signal to everyone\nelse that they think you're lame.How much should you worry about that?  The seriousness of signalling\nrisk depends on how far along you are.  If by the next time you\nneed to raise money, you have graphs showing rising revenue or\ntraffic month after month, you don't have to worry about any signals\nyour existing investors are sending.  Your results will speak for\nthemselves.  \n[7]Whereas if the next time you need to raise money you won't yet have\nconcrete results, you may need to think more about the message your\ninvestors might send if they don't invest more.  I'm not sure yet\nhow much you have to worry, because this whole phenomenon of VCs\ndoing angel investments is so new. But my instincts tell me you\ndon't have to worry much.  Signalling risk smells like one of those\nthings founders worry about that's not a real problem.  As a rule,\nthe only thing that can kill a good startup is the startup itself.\nStartups hurt themselves way more often than competitors hurt them,\nfor example.  I suspect signalling risk is in this category too.One thing YC-funded startups have been doing to mitigate the risk\nof taking money from VCs in angel rounds is not to take too much\nfrom any one VC.  Maybe that will help, if you have the luxury of\nturning down money.Fortunately, more and more startups will.  After decades of competition\nthat could best be described as intramural, the startup funding\nbusiness is finally getting some real competition.  That should\nlast several years at least, and maybe a lot longer. Unless there's\nsome huge market crash, the next couple years are going to be a\ngood time for startups to raise money.  And that's exciting because\nit means lots more startups will happen.\nNotes[1]\nI've also heard them called \"Mini-VCs\" and \"Micro-VCs.\" I\ndon't know which name will stick.There were a couple predecessors.  Ron Conway had angel funds\nstarting in the 1990s, and in some ways First Round Capital is closer to a\nsuper-angel than a VC fund.[2]\nIt wouldn't cut their overall returns tenfold, because investing\nlater would probably (a) cause them to lose less on investments\nthat failed, and (b) not allow them to get as large a percentage\nof startups as they do now.  So it's hard to predict precisely what\nwould happen to their returns.[3]\nThe brand of an investor derives mostly from the success of\ntheir portfolio companies.  The top VCs thus have a big brand\nadvantage over the super-angels.  They could make it self-perpetuating\nif they used it to get all the best new startups.  But I don't think\nthey'll be able to.  To get all the best startups, you have to do\nmore than make them want you.  You also have to want them; you have\nto recognize them when you see them, and that's much harder.\nSuper-angels will snap up stars that VCs miss.  And that will cause\nthe brand gap between the top VCs and the super-angels gradually\nto erode.[4]\nThough in a traditional series A round VCs put two partners\non your board, there are signs now that VCs may begin to conserve\nboard seats by switching to what used to be considered an angel-round\nboard, consisting of two founders and one VC.  Which is also to the\nfounders' advantage if it means they still control the company.[5]\nIn a series A round, you usually have to give up more than\nthe actual amount of stock the VCs buy, because they insist you\ndilute yourselves to set aside an \"option pool\" as well.  I predict\nthis practice will gradually disappear though.[6]\nThe best thing for founders, if they can get it, is a convertible\nnote with no valuation cap at all.  In that case the money invested\nin the angel round just converts into stock at the valuation of the\nnext round, no matter how large.  Angels and super-angels tend not\nto like uncapped notes. They have no idea how much of the company\nthey're buying.  If the company does well and the valuation of the\nnext round is high, they may end up with only a sliver of it.  So\nby agreeing to uncapped notes, VCs who don't care about valuations\nin angel rounds can make offers that super-angels hate to match.[7]\nObviously signalling risk is also not a problem if you'll\nnever need to raise more money.  But startups are often mistaken\nabout that.Thanks to Sam Altman, John Bautista, Patrick Collison, James\nLindenbaum, Reid Hoffman, Jessica Livingston and Harj Taggar\nfor reading drafts\nof this.December 2001 (rev. May 2002)\n\n(This article came about in response to some questions on\nthe LL1 mailing list.  It is now\nincorporated in Revenge of the Nerds.)When McCarthy designed Lisp in the late 1950s, it was\na radical departure from existing languages,\nthe most important of which was Fortran.Lisp embodied nine new ideas:\n1. Conditionals.  A conditional is an if-then-else\nconstruct.  We take these for granted now.  They were \ninvented\nby McCarthy in the course of developing Lisp. \n(Fortran at that time only had a conditional\ngoto, closely based on the branch instruction in the \nunderlying hardware.)  McCarthy, who was on the Algol committee, got\nconditionals into Algol, whence they spread to most other\nlanguages.2. A function type. In Lisp, functions are first class \nobjects-- they're a data type just like integers, strings,\netc, and have a literal representation, can be stored in variables,\ncan be passed as arguments, and so on.3. Recursion.  Recursion existed as a mathematical concept\nbefore Lisp of course, but Lisp was the first programming language to support\nit.  (It's arguably implicit in making functions first class\nobjects.)4. A new concept of variables.  In Lisp, all variables\nare effectively pointers. Values are what\nhave types, not variables, and assigning or binding\nvariables means copying pointers, not what they point to.5. Garbage-collection.6. Programs composed of expressions. Lisp programs are \ntrees of expressions, each of which returns a value.  \n(In some Lisps expressions\ncan return multiple values.)  This is in contrast to Fortran\nand most succeeding languages, which distinguish between\nexpressions and statements.It was natural to have this\ndistinction in Fortran because (not surprisingly in a language\nwhere the input format was punched cards) the language was\nline-oriented.  You could not nest statements.  And\nso while you needed expressions for math to work, there was\nno point in making anything else return a value, because\nthere could not be anything waiting for it.This limitation\nwent away with the arrival of block-structured languages,\nbut by then it was too late. The distinction between\nexpressions and statements was entrenched.  It spread from \nFortran into Algol and thence to both their descendants.When a language is made entirely of expressions, you can\ncompose expressions however you want.  You can say either\n(using Arc syntax)(if foo (= x 1) (= x 2))or(= x (if foo 1 2))7. A symbol type.  Symbols differ from strings in that\nyou can test equality by comparing a pointer.8. A notation for code using trees of symbols.9. The whole language always available.  \nThere is\nno real distinction between read-time, compile-time, and runtime.\nYou can compile or run code while reading, read or run code\nwhile compiling, and read or compile code at runtime.Running code at read-time lets users reprogram Lisp's syntax;\nrunning code at compile-time is the basis of macros; compiling\nat runtime is the basis of Lisp's use as an extension\nlanguage in programs like Emacs; and reading at runtime\nenables programs to communicate using s-expressions, an\nidea recently reinvented as XML.\nWhen Lisp was first invented, all these ideas were far\nremoved from ordinary programming practice, which was\ndictated largely by the hardware available in the late 1950s.Over time, the default language, embodied\nin a succession of popular languages, has\ngradually evolved toward Lisp.  1-5 are now widespread.\n6 is starting to appear in the mainstream.\nPython has a form of 7, though there doesn't seem to be\nany syntax for it.  \n8, which (with 9) is what makes Lisp macros\npossible, is so far still unique to Lisp,\nperhaps because (a) it requires those parens, or something \njust as bad, and (b) if you add that final increment of power, \nyou can no \nlonger claim to have invented a new language, but only\nto have designed a new dialect of Lisp ; -)Though useful to present-day programmers, it's\nstrange to describe Lisp in terms of its\nvariation from the random expedients other languages\nadopted.  That was not, probably, how McCarthy\nthought of it.  Lisp wasn't designed to fix the mistakes\nin Fortran; it came about more as the byproduct of an\nattempt to axiomatize computation.May 2001(This article was written as a kind of business plan for a\nnew language.\nSo it is missing (because it takes for granted) the most important\nfeature of a good programming language: very powerful abstractions.)A friend of mine once told an eminent operating systems\nexpert that he wanted to design a really good\nprogramming language.  The expert told him that it would be a\nwaste of time, that programming languages don't become popular\nor unpopular based on their merits, and so no matter how\ngood his language was, no one would use it.  At least, that\nwas what had happened to the language he had designed.What does make a language popular?  Do popular\nlanguages deserve their popularity?  Is it worth trying to\ndefine a good programming language?  How would you do it?I think the answers to these questions can be found by looking \nat hackers, and learning what they want.  Programming\nlanguages are for hackers, and a programming language\nis good as a programming language (rather than, say, an\nexercise in denotational semantics or compiler design)\nif and only if hackers like it.1 The Mechanics of PopularityIt's true, certainly, that most people don't choose programming\nlanguages simply based on their merits.  Most programmers are told\nwhat language to use by someone else.  And yet I think the effect\nof such external factors on the popularity of programming languages\nis not as great as it's sometimes thought to be. I think a bigger\nproblem is that a hacker's idea of a good programming language is\nnot the same as most language designers'.Between the two, the hacker's opinion is the one that matters.\nProgramming languages are not theorems. They're tools, designed\nfor people, and they have to be designed to suit human strengths\nand weaknesses as much as shoes have to be designed for human feet.\nIf a shoe pinches when you put it on, it's a bad shoe, however\nelegant it may be as a piece of sculpture.It may be that the majority of programmers can't tell a good language\nfrom a bad one. But that's no different with any other tool. It\ndoesn't mean that it's a waste of time to try designing a good\nlanguage. Expert hackers \ncan tell a good language when they see\none, and they'll use it. Expert hackers are a tiny minority,\nadmittedly, but that tiny minority write all the good software,\nand their influence is such that the rest of the programmers will\ntend to use whatever language they use. Often, indeed, it is not\nmerely influence but command: often the expert hackers are the very\npeople who, as their bosses or faculty advisors, tell the other\nprogrammers what language to use.The opinion of expert hackers is not the only force that determines\nthe relative popularity of programming languages \u2014 legacy software\n(Cobol) and hype (Ada, Java) also play a role \u2014 but I think it is\nthe most powerful force over the long term. Given an initial critical\nmass and enough time, a programming language probably becomes about\nas popular as it deserves to be. And popularity further separates\ngood languages from bad ones, because feedback from real live users\nalways leads to improvements. Look at how much any popular language\nhas changed during its life. Perl and Fortran are extreme cases,\nbut even Lisp has changed a lot. Lisp 1.5 didn't have macros, for\nexample; these evolved later, after hackers at MIT had spent a\ncouple years using Lisp to write real programs. [1]So whether or not a language has to be good to be popular, I think\na language has to be popular to be good. And it has to stay popular\nto stay good. The state of the art in programming languages doesn't\nstand still. And yet the Lisps we have today are still pretty much\nwhat they had at MIT in the mid-1980s, because that's the last time\nLisp had a sufficiently large and demanding user base.Of course, hackers have to know about a language before they can\nuse it. How are they to hear? From other hackers. But there has to\nbe some initial group of hackers using the language for others even\nto hear about it. I wonder how large this group has to be; how many\nusers make a critical mass? Off the top of my head, I'd say twenty.\nIf a language had twenty separate users, meaning twenty users who\ndecided on their own to use it, I'd consider it to be real.Getting there can't be easy. I would not be surprised if it is\nharder to get from zero to twenty than from twenty to a thousand.\nThe best way to get those initial twenty users is probably to use\na trojan horse: to give people an application they want, which\nhappens to be written in the new language.2 External FactorsLet's start by acknowledging one external factor that does affect\nthe popularity of a programming language. To become popular, a\nprogramming language has to be the scripting language of a popular\nsystem. Fortran and Cobol were the scripting languages of early\nIBM mainframes. C was the scripting language of Unix, and so, later,\nwas Perl. Tcl is the scripting language of Tk. Java and Javascript\nare intended to be the scripting languages of web browsers.Lisp is not a massively popular language because it is not the\nscripting language of a massively popular system. What popularity\nit retains dates back to the 1960s and 1970s, when it was the\nscripting language of MIT. A lot of the great programmers of the\nday were associated with MIT at some point. And in the early 1970s,\nbefore C, MIT's dialect of Lisp, called MacLisp, was one of the\nonly programming languages a serious hacker would want to use.Today Lisp is the scripting language of two moderately popular\nsystems, Emacs and Autocad, and for that reason I suspect that most\nof the Lisp programming done today is done in Emacs Lisp or AutoLisp.Programming languages don't exist in isolation. To hack is a\ntransitive verb \u2014 hackers are usually hacking something \u2014 and in\npractice languages are judged relative to whatever they're used to\nhack. So if you want to design a popular language, you either have\nto supply more than a language, or you have to design your language\nto replace the scripting language of some existing system.Common Lisp is unpopular partly because it's an orphan. It did\noriginally come with a system to hack: the Lisp Machine. But Lisp\nMachines (along with parallel computers) were steamrollered by the\nincreasing power of general purpose processors in the 1980s. Common\nLisp might have remained popular if it had been a good scripting\nlanguage for Unix. It is, alas, an atrociously bad one.One way to describe this situation is to say that a language isn't\njudged on its own merits. Another view is that a programming language\nreally isn't a programming language unless it's also the scripting\nlanguage of something. This only seems unfair if it comes as a\nsurprise. I think it's no more unfair than expecting a programming\nlanguage to have, say, an implementation. It's just part of what\na programming language is.A programming language does need a good implementation, of course,\nand this must be free. Companies will pay for software, but individual\nhackers won't, and it's the hackers you need to attract.A language also needs to have a book about it. The book should be\nthin, well-written, and full of good examples. K&R is the ideal\nhere. At the moment I'd almost say that a language has to have a\nbook published by O'Reilly. That's becoming the test of mattering\nto hackers.There should be online documentation as well. In fact, the book\ncan start as online documentation. But I don't think that physical\nbooks are outmoded yet. Their format is convenient, and the de\nfacto censorship imposed by publishers is a useful if imperfect\nfilter. Bookstores are one of the most important places for learning\nabout new languages.3 BrevityGiven that you can supply the three things any language needs \u2014 a\nfree implementation, a book, and something to hack \u2014 how do you\nmake a language that hackers will like?One thing hackers like is brevity. Hackers are lazy, in the same\nway that mathematicians and modernist architects are lazy: they\nhate anything extraneous. It would not be far from the truth to\nsay that a hacker about to write a program decides what language\nto use, at least subconsciously, based on the total number of\ncharacters he'll have to type. If this isn't precisely how hackers\nthink, a language designer would do well to act as if it were.It is a mistake to try to baby the user with long-winded expressions\nthat are meant to resemble English. Cobol is notorious for this\nflaw. A hacker would consider being asked to writeadd x to y giving zinstead ofz = x+yas something between an insult to his intelligence and a sin against\nGod.It has sometimes been said that Lisp should use first and rest\ninstead of car and cdr, because it would make programs easier to\nread. Maybe for the first couple hours. But a hacker can learn\nquickly enough that car means the first element of a list and cdr\nmeans the rest. Using first and rest means 50% more typing. And\nthey are also different lengths, meaning that the arguments won't\nline up when they're called, as car and cdr often are, in successive\nlines. I've found that it matters a lot how code lines up on the\npage. I can barely read Lisp code when it is set in a variable-width\nfont, and friends say this is true for other languages too.Brevity is one place where strongly typed languages lose. All other\nthings being equal, no one wants to begin a program with a bunch\nof declarations. Anything that can be implicit, should be.The individual tokens should be short as well. Perl and Common Lisp\noccupy opposite poles on this question. Perl programs can be almost\ncryptically dense, while the names of built-in Common Lisp operators\nare comically long. The designers of Common Lisp probably expected\nusers to have text editors that would type these long names for\nthem. But the cost of a long name is not just the cost of typing\nit. There is also the cost of reading it, and the cost of the space\nit takes up on your screen.4 HackabilityThere is one thing more important than brevity to a hacker: being\nable to do what you want. In the history of programming languages\na surprising amount of effort has gone into preventing programmers\nfrom doing things considered to be improper. This is a dangerously\npresumptuous plan. How can the language designer know what the\nprogrammer is going to need to do? I think language designers would\ndo better to consider their target user to be a genius who will\nneed to do things they never anticipated, rather than a bumbler\nwho needs to be protected from himself. The bumbler will shoot\nhimself in the foot anyway. You may save him from referring to\nvariables in another package, but you can't save him from writing\na badly designed program to solve the wrong problem, and taking\nforever to do it.Good programmers often want to do dangerous and unsavory things.\nBy unsavory I mean things that go behind whatever semantic facade\nthe language is trying to present: getting hold of the internal\nrepresentation of some high-level abstraction, for example. Hackers\nlike to hack, and hacking means getting inside things and second\nguessing the original designer.Let yourself be second guessed. When you make any tool, people use\nit in ways you didn't intend, and this is especially true of a\nhighly articulated tool like a programming language. Many a hacker\nwill want to tweak your semantic model in a way that you never\nimagined. I say, let them; give the programmer access to as much\ninternal stuff as you can without endangering runtime systems like\nthe garbage collector.In Common Lisp I have often wanted to iterate through the fields\nof a struct \u2014 to comb out references to a deleted object, for example,\nor find fields that are uninitialized. I know the structs are just\nvectors underneath. And yet I can't write a general purpose function\nthat I can call on any struct. I can only access the fields by\nname, because that's what a struct is supposed to mean.A hacker may only want to subvert the intended model of things once\nor twice in a big program. But what a difference it makes to be\nable to. And it may be more than a question of just solving a\nproblem. There is a kind of pleasure here too. Hackers share the\nsurgeon's secret pleasure in poking about in gross innards, the\nteenager's secret pleasure in popping zits. [2] For boys, at least,\ncertain kinds of horrors are fascinating. Maxim magazine publishes\nan annual volume of photographs, containing a mix of pin-ups and\ngrisly accidents. They know their audience.Historically, Lisp has been good at letting hackers have their way.\nThe political correctness of Common Lisp is an aberration. Early\nLisps let you get your hands on everything. A good deal of that\nspirit is, fortunately, preserved in macros. What a wonderful thing,\nto be able to make arbitrary transformations on the source code.Classic macros are a real hacker's tool \u2014 simple, powerful, and\ndangerous. It's so easy to understand what they do: you call a\nfunction on the macro's arguments, and whatever it returns gets\ninserted in place of the macro call. Hygienic macros embody the\nopposite principle. They try to protect you from understanding what\nthey're doing. I have never heard hygienic macros explained in one\nsentence. And they are a classic example of the dangers of deciding\nwhat programmers are allowed to want. Hygienic macros are intended\nto protect me from variable capture, among other things, but variable\ncapture is exactly what I want in some macros.A really good language should be both clean and dirty: cleanly\ndesigned, with a small core of well understood and highly orthogonal\noperators, but dirty in the sense that it lets hackers have their\nway with it. C is like this. So were the early Lisps. A real hacker's\nlanguage will always have a slightly raffish character.A good programming language should have features that make the kind\nof people who use the phrase \"software engineering\" shake their\nheads disapprovingly. At the other end of the continuum are languages\nlike Ada and Pascal, models of propriety that are good for teaching\nand not much else.5 Throwaway ProgramsTo be attractive to hackers, a language must be good for writing\nthe kinds of programs they want to write. And that means, perhaps\nsurprisingly, that it has to be good for writing throwaway programs.A throwaway program is a program you write quickly for some limited\ntask: a program to automate some system administration task, or\ngenerate test data for a simulation, or convert data from one format\nto another. The surprising thing about throwaway programs is that,\nlike the \"temporary\" buildings built at so many American universities\nduring World War II, they often don't get thrown away. Many evolve\ninto real programs, with real features and real users.I have a hunch that the best big programs begin life this way,\nrather than being designed big from the start, like the Hoover Dam.\nIt's terrifying to build something big from scratch. When people\ntake on a project that's too big, they become overwhelmed. The\nproject either gets bogged down, or the result is sterile and\nwooden: a shopping mall rather than a real downtown, Brasilia rather\nthan Rome, Ada rather than C.Another way to get a big program is to start with a throwaway\nprogram and keep improving it. This approach is less daunting, and\nthe design of the program benefits from evolution. I think, if one\nlooked, that this would turn out to be the way most big programs\nwere developed. And those that did evolve this way are probably\nstill written in whatever language they were first written in,\nbecause it's rare for a program to be ported, except for political\nreasons. And so, paradoxically, if you want to make a language that\nis used for big systems, you have to make it good for writing\nthrowaway programs, because that's where big systems come from.Perl is a striking example of this idea. It was not only designed\nfor writing throwaway programs, but was pretty much a throwaway\nprogram itself. Perl began life as a collection of utilities for\ngenerating reports, and only evolved into a programming language\nas the throwaway programs people wrote in it grew larger. It was\nnot until Perl 5 (if then) that the language was suitable for\nwriting serious programs, and yet it was already massively popular.What makes a language good for throwaway programs? To start with,\nit must be readily available. A throwaway program is something that\nyou expect to write in an hour. So the language probably must\nalready be installed on the computer you're using. It can't be\nsomething you have to install before you use it. It has to be there.\nC was there because it came with the operating system. Perl was\nthere because it was originally a tool for system administrators,\nand yours had already installed it.Being available means more than being installed, though. An\ninteractive language, with a command-line interface, is more\navailable than one that you have to compile and run separately. A\npopular programming language should be interactive, and start up\nfast.Another thing you want in a throwaway program is brevity. Brevity\nis always attractive to hackers, and never more so than in a program\nthey expect to turn out in an hour.6 LibrariesOf course the ultimate in brevity is to have the program already\nwritten for you, and merely to call it. And this brings us to what\nI think will be an increasingly important feature of programming\nlanguages: library functions. Perl wins because it has large\nlibraries for manipulating strings. This class of library functions\nare especially important for throwaway programs, which are often\noriginally written for converting or extracting data.  Many Perl\nprograms probably begin as just a couple library calls stuck\ntogether.I think a lot of the advances that happen in programming languages\nin the next fifty years will have to do with library functions. I\nthink future programming languages will have libraries that are as\ncarefully designed as the core language. Programming language design\nwill not be about whether to make your language strongly or weakly\ntyped, or object oriented, or functional, or whatever, but about\nhow to design great libraries. The kind of language designers who\nlike to think about how to design type systems may shudder at this.\nIt's almost like writing applications! Too bad. Languages are for\nprogrammers, and libraries are what programmers need.It's hard to design good libraries. It's not simply a matter of\nwriting a lot of code. Once the libraries get too big, it can\nsometimes take longer to find the function you need than to write\nthe code yourself. Libraries need to be designed using a small set\nof orthogonal operators, just like the core language. It ought to\nbe possible for the programmer to guess what library call will do\nwhat he needs.Libraries are one place Common Lisp falls short. There are only\nrudimentary libraries for manipulating strings, and almost none\nfor talking to the operating system. For historical reasons, Common\nLisp tries to pretend that the OS doesn't exist. And because you\ncan't talk to the OS, you're unlikely to be able to write a serious\nprogram using only the built-in operators in Common Lisp. You have\nto use some implementation-specific hacks as well, and in practice\nthese tend not to give you everything you want. Hackers would think\na lot more highly of Lisp if Common Lisp had powerful string\nlibraries and good OS support.7 SyntaxCould a language with Lisp's syntax, or more precisely, lack of\nsyntax, ever become popular? I don't know the answer to this\nquestion. I do think that syntax is not the main reason Lisp isn't\ncurrently popular. Common Lisp has worse problems than unfamiliar\nsyntax. I know several programmers who are comfortable with prefix\nsyntax and yet use Perl by default, because it has powerful string\nlibraries and can talk to the os.There are two possible problems with prefix notation: that it is\nunfamiliar to programmers, and that it is not dense enough. The\nconventional wisdom in the Lisp world is that the first problem is\nthe real one. I'm not so sure. Yes, prefix notation makes ordinary\nprogrammers panic. But I don't think ordinary programmers' opinions\nmatter. Languages become popular or unpopular based on what expert\nhackers think of them, and I think expert hackers might be able to\ndeal with prefix notation. Perl syntax can be pretty incomprehensible,\nbut that has not stood in the way of Perl's popularity. If anything\nit may have helped foster a Perl cult.A more serious problem is the diffuseness of prefix notation. For\nexpert hackers, that really is a problem. No one wants to write\n(aref a x y) when they could write a[x,y].In this particular case there is a way to finesse our way out of\nthe problem. If we treat data structures as if they were functions\non indexes, we could write (a x y) instead, which is even shorter\nthan the Perl form. Similar tricks may shorten other types of\nexpressions.We can get rid of (or make optional) a lot of parentheses by making\nindentation significant. That's how programmers read code anyway:\nwhen indentation says one thing and delimiters say another, we go\nby the indentation. Treating indentation as significant would\neliminate this common source of bugs as well as making programs\nshorter.Sometimes infix syntax is easier to read. This is especially true\nfor math expressions. I've used Lisp my whole programming life and\nI still don't find prefix math expressions natural. And yet it is\nconvenient, especially when you're generating code, to have operators\nthat take any number of arguments. So if we do have infix syntax,\nit should probably be implemented as some kind of read-macro.I don't think we should be religiously opposed to introducing syntax\ninto Lisp, as long as it translates in a well-understood way into\nunderlying s-expressions. There is already a good deal of syntax\nin Lisp. It's not necessarily bad to introduce more, as long as no\none is forced to use it. In Common Lisp, some delimiters are reserved\nfor the language, suggesting that at least some of the designers\nintended to have more syntax in the future.One of the most egregiously unlispy pieces of syntax in Common Lisp\noccurs in format strings; format is a language in its own right,\nand that language is not Lisp. If there were a plan for introducing\nmore syntax into Lisp, format specifiers might be able to be included\nin it. It would be a good thing if macros could generate format\nspecifiers the way they generate any other kind of code.An eminent Lisp hacker told me that his copy of CLTL falls open to\nthe section format. Mine too. This probably indicates room for\nimprovement. It may also mean that programs do a lot of I/O.8 EfficiencyA good language, as everyone knows, should generate fast code. But\nin practice I don't think fast code comes primarily from things\nyou do in the design of the language. As Knuth pointed out long\nago, speed only matters in certain critical bottlenecks.  And as\nmany programmers have observed since, one is very often mistaken\nabout where these bottlenecks are.So, in practice, the way to get fast code is to have a very good\nprofiler, rather than by, say, making the language strongly typed.\nYou don't need to know the type of every argument in every call in\nthe program. You do need to be able to declare the types of arguments\nin the bottlenecks. And even more, you need to be able to find out\nwhere the bottlenecks are.One complaint people have had with Lisp is that it's hard to tell\nwhat's expensive. This might be true. It might also be inevitable,\nif you want to have a very abstract language. And in any case I\nthink good profiling would go a long way toward fixing the problem:\nyou'd soon learn what was expensive.Part of the problem here is social. Language designers like to\nwrite fast compilers. That's how they measure their skill. They\nthink of the profiler as an add-on, at best. But in practice a good\nprofiler may do more to improve the speed of actual programs written\nin the language than a compiler that generates fast code. Here,\nagain, language designers are somewhat out of touch with their\nusers. They do a really good job of solving slightly the wrong\nproblem.It might be a good idea to have an active profiler \u2014 to push\nperformance data to the programmer instead of waiting for him to\ncome asking for it. For example, the editor could display bottlenecks\nin red when the programmer edits the source code. Another approach\nwould be to somehow represent what's happening in running programs.\nThis would be an especially big win in server-based applications,\nwhere you have lots of running programs to look at. An active\nprofiler could show graphically what's happening in memory as a\nprogram's running, or even make sounds that tell what's happening.Sound is a good cue to problems. In one place I worked, we had a\nbig board of dials showing what was happening to our web servers.\nThe hands were moved by little servomotors that made a slight noise\nwhen they turned. I couldn't see the board from my desk, but I\nfound that I could tell immediately, by the sound, when there was\na problem with a server.It might even be possible to write a profiler that would automatically\ndetect inefficient algorithms. I would not be surprised if certain\npatterns of memory access turned out to be sure signs of bad\nalgorithms. If there were a little guy running around inside the\ncomputer executing our programs, he would probably have as long\nand plaintive a tale to tell about his job as a federal government\nemployee. I often have a feeling that I'm sending the processor on\na lot of wild goose chases, but I've never had a good way to look\nat what it's doing.A number of Lisps now compile into byte code, which is then executed\nby an interpreter. This is usually done to make the implementation\neasier to port, but it could be a useful language feature. It might\nbe a good idea to make the byte code an official part of the\nlanguage, and to allow programmers to use inline byte code in\nbottlenecks. Then such optimizations would be portable too.The nature of speed, as perceived by the end-user, may be changing.\nWith the rise of server-based applications, more and more programs\nmay turn out to be i/o-bound. It will be worth making i/o fast.\nThe language can help with straightforward measures like simple,\nfast, formatted output functions, and also with deep structural\nchanges like caching and persistent objects.Users are interested in response time. But another kind of efficiency\nwill be increasingly important: the number of simultaneous users\nyou can support per processor. Many of the interesting applications\nwritten in the near future will be server-based, and the number of\nusers per server is the critical question for anyone hosting such\napplications. In the capital cost of a business offering a server-based\napplication, this is the divisor.For years, efficiency hasn't mattered much in most end-user\napplications. Developers have been able to assume that each user\nwould have an increasingly powerful processor sitting on their\ndesk. And by Parkinson's Law, software has expanded to use the\nresources available. That will change with server-based applications.\nIn that world, the hardware and software will be supplied together.\nFor companies that offer server-based applications, it will make\na very big difference to the bottom line how many users they can\nsupport per server.In some applications, the processor will be the limiting factor,\nand execution speed will be the most important thing to optimize.\nBut often memory will be the limit; the number of simultaneous\nusers will be determined by the amount of memory you need for each\nuser's data. The language can help here too. Good support for\nthreads will enable all the users to share a single heap. It may\nalso help to have persistent objects and/or language level support\nfor lazy loading.9 TimeThe last ingredient a popular language needs is time. No one wants\nto write programs in a language that might go away, as so many\nprogramming languages do. So most hackers will tend to wait until\na language has been around for a couple years before even considering\nusing it.Inventors of wonderful new things are often surprised to discover\nthis, but you need time to get any message through to people. A\nfriend of mine rarely does anything the first time someone asks\nhim. He knows that people sometimes ask for things that they turn\nout not to want. To avoid wasting his time, he waits till the third\nor fourth time he's asked to do something; by then, whoever's asking\nhim may be fairly annoyed, but at least they probably really do\nwant whatever they're asking for.Most people have learned to do a similar sort of filtering on new\nthings they hear about. They don't even start paying attention\nuntil they've heard about something ten times. They're perfectly\njustified: the majority of hot new whatevers do turn out to be a\nwaste of time, and eventually go away. By delaying learning VRML,\nI avoided having to learn it at all.So anyone who invents something new has to expect to keep repeating\ntheir message for years before people will start to get it. We\nwrote what was, as far as I know, the first web-server based\napplication, and it took us years to get it through to people that\nit didn't have to be downloaded. It wasn't that they were stupid.\nThey just had us tuned out.The good news is, simple repetition solves the problem. All you\nhave to do is keep telling your story, and eventually people will\nstart to hear. It's not when people notice you're there that they\npay attention; it's when they notice you're still there.It's just as well that it usually takes a while to gain momentum.\nMost technologies evolve a good deal even after they're first\nlaunched \u2014 programming languages especially. Nothing could be better,\nfor a new techology, than a few years of being used only by a small\nnumber of early adopters. Early adopters are sophisticated and\ndemanding, and quickly flush out whatever flaws remain in your\ntechnology. When you only have a few users you can be in close\ncontact with all of them. And early adopters are forgiving when\nyou improve your system, even if this causes some breakage.There are two ways new technology gets introduced: the organic\ngrowth method, and the big bang method. The organic growth method\nis exemplified by the classic seat-of-the-pants underfunded garage\nstartup. A couple guys, working in obscurity, develop some new\ntechnology. They launch it with no marketing and initially have\nonly a few (fanatically devoted) users. They continue to improve\nthe technology, and meanwhile their user base grows by word of\nmouth. Before they know it, they're big.The other approach, the big bang method, is exemplified by the\nVC-backed, heavily marketed startup. They rush to develop a product,\nlaunch it with great publicity, and immediately (they hope) have\na large user base.Generally, the garage guys envy the big bang guys. The big bang\nguys are smooth and confident and respected by the VCs. They can\nafford the best of everything, and the PR campaign surrounding the\nlaunch has the side effect of making them celebrities. The organic\ngrowth guys, sitting in their garage, feel poor and unloved. And\nyet I think they are often mistaken to feel sorry for themselves.\nOrganic growth seems to yield better technology and richer founders\nthan the big bang method. If you look at the dominant technologies\ntoday, you'll find that most of them grew organically.This pattern doesn't only apply to companies. You see it in sponsored\nresearch too. Multics and Common Lisp were big-bang projects, and\nUnix and MacLisp were organic growth projects.10 Redesign\"The best writing is rewriting,\" wrote E. B. White.  Every good\nwriter knows this, and it's true for software too. The most important\npart of design is redesign. Programming languages, especially,\ndon't get redesigned enough.To write good software you must simultaneously keep two opposing\nideas in your head. You need the young hacker's naive faith in\nhis abilities, and at the same time the veteran's skepticism. You\nhave to be able to think \nhow hard can it be? with one half of\nyour brain while thinking \nit will never work with the other.The trick is to realize that there's no real contradiction here.\nYou want to be optimistic and skeptical about two different things.\nYou have to be optimistic about the possibility of solving the\nproblem, but skeptical about the value of whatever solution you've\ngot so far.People who do good work often think that whatever they're working\non is no good. Others see what they've done and are full of wonder,\nbut the creator is full of worry. This pattern is no coincidence:\nit is the worry that made the work good.If you can keep hope and worry balanced, they will drive a project\nforward the same way your two legs drive a bicycle forward. In the\nfirst phase of the two-cycle innovation engine, you work furiously\non some problem, inspired by your confidence that you'll be able\nto solve it. In the second phase, you look at what you've done in\nthe cold light of morning, and see all its flaws very clearly. But\nas long as your critical spirit doesn't outweigh your hope, you'll\nbe able to look at your admittedly incomplete system, and think,\nhow hard can it be to get the rest of the way?, thereby continuing\nthe cycle.It's tricky to keep the two forces balanced. In young hackers,\noptimism predominates. They produce something, are convinced it's\ngreat, and never improve it. In old hackers, skepticism predominates,\nand they won't even dare to take on ambitious projects.Anything you can do to keep the redesign cycle going is good. Prose\ncan be rewritten over and over until you're happy with it. But\nsoftware, as a rule, doesn't get redesigned enough. Prose has\nreaders, but software has users. If a writer rewrites an essay,\npeople who read the old version are unlikely to complain that their\nthoughts have been broken by some newly introduced incompatibility.Users are a double-edged sword. They can help you improve your\nlanguage, but they can also deter you from improving it. So choose\nyour users carefully, and be slow to grow their number. Having\nusers is like optimization: the wise course is to delay it. Also,\nas a general rule, you can at any given time get away with changing\nmore than you think. Introducing change is like pulling off a\nbandage: the pain is a memory almost as soon as you feel it.Everyone knows that it's not a good idea to have a language designed\nby a committee. Committees yield bad design. But I think the worst\ndanger of committees is that they interfere with redesign. It is\nso much work to introduce changes that no one wants to bother.\nWhatever a committee decides tends to stay that way, even if most\nof the members don't like it.Even a committee of two gets in the way of redesign. This happens\nparticularly in the interfaces between pieces of software written\nby two different people. To change the interface both have to agree\nto change it at once. And so interfaces tend not to change at all,\nwhich is a problem because they tend to be one of the most ad hoc\nparts of any system.One solution here might be to design systems so that interfaces\nare horizontal instead of vertical \u2014 so that modules are always\nvertically stacked strata of abstraction. Then the interface will\ntend to be owned by one of them. The lower of two levels will either\nbe a language in which the upper is written, in which case the\nlower level will own the interface, or it will be a slave, in which\ncase the interface can be dictated by the upper level.11 LispWhat all this implies is that there is hope for a new Lisp.  There\nis hope for any language that gives hackers what they want, including\nLisp. I think we may have made a mistake in thinking that hackers\nare turned off by Lisp's strangeness. This comforting illusion may\nhave prevented us from seeing the real problem with Lisp, or at\nleast Common Lisp, which is that it sucks for doing what hackers\nwant to do. A hacker's language needs powerful libraries and\nsomething to hack. Common Lisp has neither. A hacker's language is\nterse and hackable. Common Lisp is not.The good news is, it's not Lisp that sucks, but Common Lisp. If we\ncan develop a new Lisp that is a real hacker's language, I think\nhackers will use it. They will use whatever language does the job.\nAll we have to do is make sure this new Lisp does some important\njob better than other languages.History offers some encouragement. Over time, successive new\nprogramming languages have taken more and more features from Lisp.\nThere is no longer much left to copy before the language you've\nmade is Lisp. The latest hot language, Python, is a watered-down\nLisp with infix syntax and no macros. A new Lisp would be a natural\nstep in this progression.I sometimes think that it would be a good marketing trick to call\nit an improved version of Python. That sounds hipper than Lisp. To\nmany people, Lisp is a slow AI language with a lot of parentheses.\nFritz Kunze's official biography carefully avoids mentioning the\nL-word.  But my guess is that we shouldn't be afraid to call the\nnew Lisp Lisp. Lisp still has a lot of latent respect among the\nvery best hackers \u2014 the ones who took 6.001 and understood it, for\nexample. And those are the users you need to win.In \"How to Become a Hacker,\" Eric Raymond describes Lisp as something\nlike Latin or Greek \u2014 a language you should learn as an intellectual\nexercise, even though you won't actually use it:\n\n  Lisp is worth learning for the profound enlightenment experience\n  you will have when you finally get it; that experience will make\n  you a better programmer for the rest of your days, even if you\n  never actually use Lisp itself a lot.\n\nIf I didn't know Lisp, reading this would set me asking questions.\nA language that would make me a better programmer, if it means\nanything at all, means a language that would be better for programming.\nAnd that is in fact the implication of what Eric is saying.As long as that idea is still floating around, I think hackers will\nbe receptive enough to a new Lisp, even if it is called Lisp. But\nthis Lisp must be a hacker's language, like the classic Lisps of\nthe 1970s. It must be terse, simple, and hackable. And it must have\npowerful libraries for doing what hackers want to do now.In the matter of libraries I think there is room to beat languages\nlike Perl and Python at their own game. A lot of the new applications\nthat will need to be written in the coming years will be \nserver-based\napplications. There's no reason a new Lisp shouldn't have string\nlibraries as good as Perl, and if this new Lisp also had powerful\nlibraries for server-based applications, it could be very popular.\nReal hackers won't turn up their noses at a new tool that will let\nthem solve hard problems with a few library calls. Remember, hackers\nare lazy.It could be an even bigger win to have core language support for\nserver-based applications. For example, explicit support for programs\nwith multiple users, or data ownership at the level of type tags.Server-based applications also give us the answer to the question\nof what this new Lisp will be used to hack. It would not hurt to\nmake Lisp better as a scripting language for Unix. (It would be\nhard to make it worse.) But I think there are areas where existing\nlanguages would be easier to beat. I think it might be better to\nfollow the model of Tcl, and supply the Lisp together with a complete\nsystem for supporting server-based applications. Lisp is a natural\nfit for server-based applications. Lexical closures provide a way\nto get the effect of subroutines when the ui is just a series of\nweb pages. S-expressions map nicely onto html, and macros are good\nat generating it. There need to be better tools for writing\nserver-based applications, and there needs to be a new Lisp, and\nthe two would work very well together.12 The Dream LanguageBy way of summary, let's try describing the hacker's dream language.\nThe dream language is \nbeautiful, clean, and terse. It has an\ninteractive toplevel that starts up fast. You can write programs\nto solve common problems with very little code.  Nearly all the\ncode in any program you write is code that's specific to your\napplication. Everything else has been done for you.The syntax of the language is brief to a fault. You never have to\ntype an unnecessary character, or even to use the shift key much.Using big abstractions you can write the first version of a program\nvery quickly. Later, when you want to optimize, there's a really\ngood profiler that tells you where to focus your attention. You\ncan make inner loops blindingly fast, even writing inline byte code\nif you need to.There are lots of good examples to learn from, and the language is\nintuitive enough that you can learn how to use it from examples in\na couple minutes. You don't need to look in the manual much. The\nmanual is thin, and has few warnings and qualifications.The language has a small core, and powerful, highly orthogonal\nlibraries that are as carefully designed as the core language. The\nlibraries all work well together; everything in the language fits\ntogether like the parts in a fine camera. Nothing is deprecated,\nor retained for compatibility. The source code of all the libraries\nis readily available. It's easy to talk to the operating system\nand to applications written in other languages.The language is built in layers. The higher-level abstractions are\nbuilt in a very transparent way out of lower-level abstractions,\nwhich you can get hold of if you want.Nothing is hidden from you that doesn't absolutely have to be. The\nlanguage offers abstractions only as a way of saving you work,\nrather than as a way of telling you what to do. In fact, the language\nencourages you to be an equal participant in its design. You can\nchange everything about it, including even its syntax, and anything\nyou write has, as much as possible, the same status as what comes\npredefined.Notes[1]  Macros very close to the modern idea were proposed by Timothy\nHart in 1964, two years after Lisp 1.5 was released. What was\nmissing, initially, were ways to avoid variable capture and multiple\nevaluation; Hart's examples are subject to both.[2]  In When the Air Hits Your Brain, neurosurgeon Frank Vertosick\nrecounts a conversation in which his chief resident, Gary, talks\nabout the difference between surgeons and internists (\"fleas\"):\n\n  Gary and I ordered a large pizza and found an open booth. The\n  chief lit a cigarette. \"Look at those goddamn fleas, jabbering\n  about some disease they'll see once in their lifetimes. That's\n  the trouble with fleas, they only like the bizarre stuff. They\n  hate their bread and butter cases. That's the difference between\n  us and the fucking fleas. See, we love big juicy lumbar disc\n  herniations, but they hate hypertension....\"\n\nIt's hard to think of a lumbar disc herniation as juicy (except\nliterally). And yet I think I know what they mean. I've often had\na juicy bug to track down. Someone who's not a programmer would\nfind it hard to imagine that there could be pleasure in a bug.\nSurely it's better if everything just works. In one way, it is.\nAnd yet there is undeniably a grim satisfaction in hunting down\ncertain sorts of bugs.November 2021(This essay is derived from a talk at the Cambridge Union.)When I was a kid, I'd have said there wasn't. My father told me so.\nSome people like some things, and other people like other things,\nand who's to say who's right?It seemed so obvious that there was no such thing as good taste\nthat it was only through indirect evidence that I realized my father\nwas wrong. And that's what I'm going to give you here: a proof by\nreductio ad absurdum. If we start from the premise that there's no\nsuch thing as good taste, we end up with conclusions that are\nobviously false, and therefore the premise must be wrong.We'd better start by saying what good taste is. There's a narrow\nsense in which it refers to aesthetic judgements and a broader one\nin which it refers to preferences of any kind. The strongest proof\nwould be to show that taste exists in the narrowest sense, so I'm\ngoing to talk about taste in art. You have better taste than me if\nthe art you like is better than the art I like.If there's no such thing as good taste, then there's no such thing\nas good art. Because if there is such a\nthing as good art, it's\neasy to tell which of two people has better taste. Show them a lot\nof works by artists they've never seen before and ask them to\nchoose the best, and whoever chooses the better art has better\ntaste.So if you want to discard the concept of good taste, you also have\nto discard the concept of good art. And that means you have to\ndiscard the possibility of people being good at making it. Which\nmeans there's no way for artists to be good at their jobs. And not\njust visual artists, but anyone who is in any sense an artist. You\ncan't have good actors, or novelists, or composers, or dancers\neither. You can have popular novelists, but not good ones.We don't realize how far we'd have to go if we discarded the concept\nof good taste, because we don't even debate the most obvious cases.\nBut it doesn't just mean we can't say which of two famous painters\nis better. It means we can't say that any painter is better than a\nrandomly chosen eight year old.That was how I realized my father was wrong. I started studying\npainting. And it was just like other kinds of work I'd done: you\ncould do it well, or badly, and if you tried hard, you could get\nbetter at it. And it was obvious that Leonardo and Bellini were\nmuch better at it than me. That gap between us was not imaginary.\nThey were so good. And if they could be good, then art could be\ngood, and there was such a thing as good taste after all.Now that I've explained how to show there is such a thing as good\ntaste, I should also explain why people think there isn't. There\nare two reasons. One is that there's always so much disagreement\nabout taste. Most people's response to art is a tangle of unexamined\nimpulses. Is the artist famous? Is the subject attractive? Is this\nthe sort of art they're supposed to like? Is it hanging in a famous\nmuseum, or reproduced in a big, expensive book? In practice most\npeople's response to art is dominated by such extraneous factors.And the people who do claim to have good taste are so often mistaken.\nThe paintings admired by the so-called experts in one generation\nare often so different from those admired a few generations later.\nIt's easy to conclude there's nothing real there at all. It's only\nwhen you isolate this force, for example by trying to paint and\ncomparing your work to Bellini's, that you can see that it does in\nfact exist.The other reason people doubt that art can be good is that there\ndoesn't seem to be any room in the art for this goodness. The\nargument goes like this. Imagine several people looking at a work\nof art and judging how good it is. If being good art really is a\nproperty of objects, it should be in the object somehow. But it\ndoesn't seem to be; it seems to be something happening in the heads\nof each of the observers. And if they disagree, how do you choose\nbetween them?The solution to this puzzle is to realize that the purpose of art\nis to work on its human audience, and humans have a lot in common.\nAnd to the extent the things an object acts upon respond in the\nsame way, that's arguably what it means for the object to have the\ncorresponding property. If everything a particle interacts with\nbehaves as if the particle had a mass of m, then it has a mass of\nm. So the distinction between \"objective\" and \"subjective\" is not\nbinary, but a matter of degree, depending on how much the subjects\nhave in common. Particles interacting with one another are at one\npole, but people interacting with art are not all the way at the\nother; their reactions aren't random.Because people's responses to art aren't random, art can be designed\nto operate on people, and be good or bad depending on how effectively\nit does so. Much as a vaccine can be. If someone were talking about\nthe ability of a vaccine to confer immunity, it would seem very\nfrivolous to object that conferring immunity wasn't really a property\nof vaccines, because acquiring immunity is something that happens\nin the immune system of each individual person. Sure, people's\nimmune systems vary, and a vaccine that worked on one might not\nwork on another, but that doesn't make it meaningless to talk about\nthe effectiveness of a vaccine.The situation with art is messier, of course. You can't measure\neffectiveness by simply taking a vote, as you do with vaccines.\nYou have to imagine the responses of subjects with a deep knowledge\nof art, and enough clarity of mind to be able to ignore extraneous\ninfluences like the fame of the artist. And even then you'd still\nsee some disagreement. People do vary, and judging art is hard,\nespecially recent art. There is definitely not a total order either\nof works or of people's ability to judge them. But there is equally\ndefinitely a partial order of both. So while it's not possible to\nhave perfect taste, it is possible to have good taste.\nThanks to the Cambridge Union for inviting me, and to Trevor\nBlackwell, Jessica Livingston, and Robert Morris for reading drafts\nof this.\nJanuary 2003(This article is derived from a keynote talk at the fall 2002 meeting\nof NEPLS.)Visitors to this country are often surprised to find that\nAmericans like to begin a conversation by asking \"what do you do?\"\nI've never liked this question.  I've rarely had a\nneat answer to it.  But I think I have finally solved the problem.\nNow, when someone asks me what I do, I look them straight\nin the eye and say \"I'm designing a \nnew dialect of Lisp.\"   \nI recommend this answer to anyone who doesn't like being asked what\nthey do.  The conversation will turn immediately to other topics.I don't consider myself to be doing research on programming languages.\nI'm just designing one, in the same way that someone might design\na building or a chair or a new typeface.\nI'm not trying to discover anything new.  I just want\nto make a language that will be good to program in.  In some ways,\nthis assumption makes life a lot easier.The difference between design and research seems to be a question\nof new versus good.  Design doesn't have to be new, but it has to  \nbe good.  Research doesn't have to be good, but it has to be new.\nI think these two paths converge at the top: the best design\nsurpasses its predecessors by using new ideas, and the best research\nsolves problems that are not only new, but actually worth solving.\nSo ultimately we're aiming for the same destination, just approaching\nit from different directions.What I'm going to talk about today is what your target looks like\nfrom the back.  What do you do differently when you treat\nprogramming languages as a design problem instead of a research topic?The biggest difference is that you focus more on the user.\nDesign begins by asking, who is this\nfor and what do they need from it?  A good architect,\nfor example, does not begin by creating a design that he then\nimposes on the users, but by studying the intended users and figuring\nout what they need.Notice I said \"what they need,\" not \"what they want.\"  I don't mean\nto give the impression that working as a designer means working as \na sort of short-order cook, making whatever the client tells you\nto.  This varies from field to field in the arts, but\nI don't think there is any field in which the best work is done by\nthe people who just make exactly what the customers tell them to.The customer is always right in\nthe sense that the measure of good design is how well it works\nfor the user.  If you make a novel that bores everyone, or a chair\nthat's horribly uncomfortable to sit in, then you've done a bad\njob, period.  It's no defense to say that the novel or the chair  \nis designed according to the most advanced theoretical principles.And yet, making what works for the user doesn't mean simply making\nwhat the user tells you to.  Users don't know what all the choices\nare, and are often mistaken about what they really want.The answer to the paradox, I think, is that you have to design\nfor the user, but you have to design what the user needs, not simply  \nwhat he says he wants.\nIt's much like being a doctor.  You can't just treat a patient's\nsymptoms.  When a patient tells you his symptoms, you have to figure\nout what's actually wrong with him, and treat that.This focus on the user is a kind of axiom from which most of the\npractice of good design can be derived, and around which most design\nissues center.If good design must do what the user needs, who is the user?  When\nI say that design must be for users, I don't mean to imply that good \ndesign aims at some kind of  \nlowest common denominator.  You can pick any group of users you\nwant.  If you're designing a tool, for example, you can design it\nfor anyone from beginners to experts, and what's good design\nfor one group might be bad for another.  The point\nis, you have to pick some group of users.  I don't think you can\neven talk about good or bad design except with\nreference to some intended user.You're most likely to get good design if the intended users include\nthe designer himself.  When you design something\nfor a group that doesn't include you, it tends to be for people\nyou consider to be less sophisticated than you, not more sophisticated.That's a problem, because looking down on the user, however benevolently,\nseems inevitably to corrupt the designer.\nI suspect that very few housing\nprojects in the US were designed by architects who expected to live\nin them.   You can see the same thing\nin programming languages.  C, Lisp, and Smalltalk were created for\ntheir own designers to use.  Cobol, Ada, and Java, were created   \nfor other people to use.If you think you're designing something for idiots, the odds are\nthat you're not designing something good, even for idiots.\nEven if you're designing something for the most sophisticated\nusers, though, you're still designing for humans.  It's different \nin research.  In math you\ndon't choose abstractions because they're\neasy for humans to understand; you choose whichever make the\nproof shorter.  I think this is true for the sciences generally.\nScientific ideas are not meant to be ergonomic.Over in the arts, things are very different.  Design is\nall about people.  The human body is a strange\nthing, but when you're designing a chair,\nthat's what you're designing for, and there's no way around it.\nAll the arts have to pander to the interests and limitations\nof humans.   In painting, for example, all other things being\nequal a painting with people in it will be more interesting than\none without.  It is not merely an accident of history that\nthe great paintings of the Renaissance are all full of people.\nIf they hadn't been, painting as a medium wouldn't have the prestige\nthat it does.Like it or not, programming languages are also for people,\nand I suspect the human brain is just as lumpy and idiosyncratic\nas the human body.  Some ideas are easy for people to grasp\nand some aren't.  For example, we seem to have a very limited\ncapacity for dealing with detail.  It's this fact that makes\nprograming languages a good idea in the first place; if we\ncould handle the detail, we could just program in machine\nlanguage.Remember, too, that languages are not\nprimarily a form for finished programs, but something that\nprograms have to be developed in.  Anyone in the arts could\ntell you that you might want different mediums for the\ntwo situations.  Marble, for example, is a nice, durable\nmedium for finished ideas, but a hopelessly inflexible one\nfor developing new ideas.A program, like a proof,\nis a pruned version of a tree that in the past has had\nfalse starts branching off all over it.  So the test of\na language is not simply how clean the finished program looks\nin it, but how clean the path to the finished program was.\nA design choice that gives you elegant finished programs\nmay not give you an elegant design process.  For example, \nI've written a few macro-defining macros full of nested\nbackquotes that look now like little gems, but writing them\ntook hours of the ugliest trial and error, and frankly, I'm still\nnot entirely sure they're correct.We often act as if the test of a language were how good\nfinished programs look in it.\nIt seems so convincing when you see the same program\nwritten in two languages, and one version is much shorter.\nWhen you approach the problem from the direction of the\narts, you're less likely to depend on this sort of\ntest.  You don't want to end up with a programming\nlanguage like marble.For example, it is a huge win in developing software to\nhave an interactive toplevel, what in Lisp is called a\nread-eval-print loop.  And when you have one this has\nreal effects on the design of the language.  It would not\nwork well for a language where you have to declare\nvariables before using them, for example.  When you're\njust typing expressions into the toplevel, you want to be \nable to set x to some value and then start doing things\nto x.  You don't want to have to declare the type of x\nfirst.  You may dispute either of the premises, but if\na language has to have a toplevel to be convenient, and\nmandatory type declarations are incompatible with a\ntoplevel, then no language that makes type declarations  \nmandatory could be convenient to program in.In practice, to get good design you have to get close, and stay\nclose, to your users.  You have to calibrate your ideas on actual\nusers constantly, especially in the beginning.  One of the reasons\nJane Austen's novels are so good is that she read them out loud to\nher family.  That's why she never sinks into self-indulgently arty\ndescriptions of landscapes,\nor pretentious philosophizing.  (The philosophy's there, but it's\nwoven into the story instead of being pasted onto it like a label.)\nIf you open an average \"literary\" novel and imagine reading it out loud\nto your friends as something you'd written, you'll feel all too\nkeenly what an imposition that kind of thing is upon the reader.In the software world, this idea is known as Worse is Better.\nActually, there are several ideas mixed together in the concept of\nWorse is Better, which is why people are still arguing about\nwhether worse\nis actually better or not.  But one of the main ideas in that\nmix is that if you're building something new, you should get a\nprototype in front of users as soon as possible.The alternative approach might be called the Hail Mary strategy.\nInstead of getting a prototype out quickly and gradually refining\nit, you try to create the complete, finished, product in one long\ntouchdown pass.  As far as I know, this is a\nrecipe for disaster.  Countless startups destroyed themselves this\nway during the Internet bubble.  I've never heard of a case\nwhere it worked.What people outside the software world may not realize is that\nWorse is Better is found throughout the arts.\nIn drawing, for example, the idea was discovered during the\nRenaissance.  Now almost every drawing teacher will tell you that\nthe right way to get an accurate drawing is not to\nwork your way slowly around the contour of an object, because errors will\naccumulate and you'll find at the end that the lines don't meet.\nInstead you should draw a few quick lines in roughly the right place,\nand then gradually refine this initial sketch.In most fields, prototypes\nhave traditionally been made out of different materials.\nTypefaces to be cut in metal were initially designed  \nwith a brush on paper.  Statues to be cast in bronze   \nwere modelled in wax.  Patterns to be embroidered on tapestries\nwere drawn on paper with ink wash.  Buildings to be\nconstructed from stone were tested on a smaller scale in wood.What made oil paint so exciting, when it\nfirst became popular in the fifteenth century, was that you\ncould actually make the finished work from the prototype.\nYou could make a preliminary drawing if you wanted to, but you\nweren't held to it; you could work out all the details, and\neven make major changes, as you finished the painting.You can do this in software too.  A prototype doesn't have to\nbe just a model; you can refine it into the finished product.\nI think you should always do this when you can.  It lets you\ntake advantage of new insights you have along the way.  But\nperhaps even more important, it's good for morale.Morale is key in design.  I'm surprised people\ndon't talk more about it.  One of my first\ndrawing teachers told me: if you're bored when you're\ndrawing something, the drawing will look boring.\nFor example, suppose you have to draw a building, and you\ndecide to draw each brick individually.  You can do this\nif you want, but if you get bored halfway through and start\nmaking the bricks mechanically instead of observing each one,   \nthe drawing will look worse than if you had merely suggested\nthe bricks.Building something by gradually refining a prototype is good\nfor morale because it keeps you engaged.  In software, my  \nrule is: always have working code.  If you're writing\nsomething that you'll be able to test in an hour, then you\nhave the prospect of an immediate reward to motivate you.\nThe same is true in the arts, and particularly in oil painting.\nMost painters start with a blurry sketch and gradually\nrefine it.\nIf you work this way, then in principle\nyou never have to end the day with something that actually\nlooks unfinished.  Indeed, there is even a saying among\npainters: \"A painting is never finished, you just stop\nworking on it.\"  This idea will be familiar to anyone who\nhas worked on software.Morale is another reason that it's hard to design something\nfor an unsophisticated user.   It's hard to stay interested in\nsomething you don't like yourself.  To make something  \ngood, you have to be thinking, \"wow, this is really great,\"\nnot \"what a piece of shit; those fools will love it.\"Design means making things for humans.  But it's not just the\nuser who's human.  The designer is human too.Notice all this time I've been talking about \"the designer.\"\nDesign usually has to be under the control of a single person to\nbe any good.   And yet it seems to be possible for several people\nto collaborate on a research project.  This seems to\nme one of the most interesting differences between research and\ndesign.There have been famous instances of collaboration in the arts,\nbut most of them seem to have been cases of molecular bonding rather\nthan nuclear fusion.  In an opera it's common for one person to\nwrite the libretto and another to write the music.   And during the Renaissance, \njourneymen from northern\nEurope were often employed to do the landscapes in the\nbackgrounds of Italian paintings.  But these aren't true collaborations.\nThey're more like examples of Robert Frost's\n\"good fences make good neighbors.\"  You can stick instances\nof good design together, but within each individual project,\none person has to be in control.I'm not saying that good design requires that one person think\nof everything.  There's nothing more valuable than the advice\nof someone whose judgement you trust.  But after the talking is\ndone, the decision about what to do has to rest with one person.Why is it that research can be done by collaborators and  \ndesign can't?  This is an interesting question.  I don't \nknow the answer.  Perhaps,\nif design and research converge, the best research is also\ngood design, and in fact can't be done by collaborators.\nA lot of the most famous scientists seem to have worked alone.\nBut I don't know enough to say whether there\nis a pattern here.  It could be simply that many famous scientists\nworked when collaboration was less common.Whatever the story is in the sciences, true collaboration\nseems to be vanishingly rare in the arts.  Design by committee is a\nsynonym for bad design.  Why is that so?  Is there some way to\nbeat this limitation?I'm inclined to think there isn't-- that good design requires\na dictator.  One reason is that good design has to   \nbe all of a piece.  Design is not just for humans, but\nfor individual humans.  If a design represents an idea that  \nfits in one person's head, then the idea will fit in the user's\nhead too.Related:April 2004To the popular press, \"hacker\" means someone who breaks\ninto computers.  Among programmers it means a good programmer.\nBut the two meanings are connected.  To programmers,\n\"hacker\" connotes mastery in the most literal sense: someone\nwho can make a computer do what he wants\u2014whether the computer\nwants to or not.To add to the confusion, the noun \"hack\" also has two senses.  It can\nbe either a compliment or an insult.  It's called a hack when\nyou do something in an ugly way.  But when you do something\nso clever that you somehow beat the system, that's also\ncalled a hack.  The word is used more often in the former than\nthe latter sense, probably because ugly solutions are more\ncommon than brilliant ones.Believe it or not, the two senses of \"hack\" are also\nconnected.  Ugly and imaginative solutions have something in\ncommon: they both break the rules.  And there is a gradual\ncontinuum between rule breaking that's merely ugly (using\nduct tape to attach something to your bike) and rule breaking\nthat is brilliantly imaginative (discarding Euclidean space).Hacking predates computers.  When he\nwas working on the Manhattan Project, Richard Feynman used to\namuse himself by breaking into safes containing secret documents.\nThis tradition continues today.\nWhen we were in grad school, a hacker friend of mine who spent too much\ntime around MIT had\nhis own lock picking kit.\n(He now runs a hedge fund, a not unrelated enterprise.)It is sometimes hard to explain to authorities why one would\nwant to do such things.\nAnother friend of mine once got in trouble with the government for\nbreaking into computers.  This had only recently been declared\na crime, and the FBI found that their usual investigative\ntechnique didn't work.  Police investigation apparently begins with\na motive.  The usual motives are few: drugs, money, sex,\nrevenge.  Intellectual curiosity was not one of the motives on\nthe FBI's list.  Indeed, the whole concept seemed foreign to\nthem.Those in authority tend to be annoyed by hackers'\ngeneral attitude of disobedience.  But that disobedience is\na byproduct of the qualities that make them good programmers.\nThey may laugh at the CEO when he talks in generic corporate\nnewspeech, but they also laugh at someone who tells them\na certain problem can't be solved.\nSuppress one, and you suppress the other.This attitude is sometimes affected.  Sometimes young programmers\nnotice the eccentricities of eminent hackers and decide to\nadopt some of their own in order to seem smarter.\nThe fake version is not merely\nannoying; the prickly attitude of these posers\ncan actually slow the process of innovation.But even factoring in their annoying eccentricities,\nthe disobedient attitude of hackers is a net win.  I wish its\nadvantages were better understood.For example, I suspect people in Hollywood are\nsimply mystified by\nhackers' attitudes toward copyrights.  They are a perennial\ntopic of heated discussion on Slashdot.\nBut why should people who program computers\nbe so concerned about copyrights, of all things?Partly because some companies use mechanisms to prevent\ncopying.  Show any hacker a lock and his first thought is\nhow to pick it.  But there is a deeper reason that\nhackers are alarmed by measures like copyrights and patents.\nThey see increasingly aggressive measures to protect\n\"intellectual property\"\nas a threat to the intellectual\nfreedom they need to do their job.\nAnd they are right.It is by poking about inside current technology that\nhackers get ideas for the next generation.  No thanks,\nintellectual homeowners may say, we don't need any\noutside help.  But they're wrong.\nThe next generation of computer technology has\noften\u2014perhaps more often than not\u2014been developed by outsiders.In 1977 there was no doubt some group within IBM developing\nwhat they expected to be\nthe next generation of business computer.  They were mistaken.\nThe next generation of business computer was\nbeing developed on entirely different lines by two long-haired\nguys called Steve in a garage in Los Altos.  At about the\nsame time, the powers that be\nwere cooperating to develop the\nofficial next generation operating system, Multics.\nBut two guys who thought Multics excessively complex went off\nand wrote their own.  They gave it a name that\nwas a joking reference to Multics: Unix.The latest intellectual property laws impose\nunprecedented restrictions on the sort of poking around that\nleads to new ideas. In the past, a competitor might use patents\nto prevent you from selling a copy of something they\nmade, but they couldn't prevent you from\ntaking one apart to see how it worked.   The latest\nlaws make this a crime.  How are we\nto develop new technology if we can't study current\ntechnology to figure out how to improve it?Ironically, hackers have brought this on themselves.\nComputers are responsible for the problem.  The control systems\ninside machines used to be physical: gears and levers and cams.\nIncreasingly, the brains (and thus the value) of products is\nin software. And by this I mean software in the general sense:\ni.e. data.  A song on an LP is physically stamped into the\nplastic.  A song on an iPod's disk is merely stored on it.Data is by definition easy to copy.  And the Internet\nmakes copies easy to distribute.  So it is no wonder\ncompanies are afraid.  But, as so often happens, fear has\nclouded their judgement.  The government has responded\nwith draconian laws to protect intellectual property.\nThey probably mean well. But\nthey may not realize that such laws will do more harm\nthan good.Why are programmers so violently opposed to these laws?\nIf I were a legislator, I'd be interested in this\nmystery\u2014for the same reason that, if I were a farmer and suddenly\nheard a lot of squawking coming from my hen house one night,\nI'd want to go out and investigate.  Hackers are not stupid,\nand unanimity is very rare in this world.\nSo if they're all squawking,   \nperhaps there is something amiss.Could it be that such laws, though intended to protect America,\nwill actually harm it?  Think about it.  There is something\nvery American about Feynman breaking into safes during\nthe Manhattan Project.  It's hard to imagine the authorities\nhaving a sense of humor about such things over\nin Germany at that time.  Maybe it's not a coincidence.Hackers are unruly.  That is the essence of hacking.  And it\nis also the essence of Americanness.  It is no accident\nthat Silicon Valley\nis in America, and not France, or Germany,\nor England, or Japan. In those countries, people color inside\nthe lines.I lived for a while in Florence.  But after I'd been there\na few months I realized that what I'd been unconsciously hoping\nto find there was back in the place I'd just left.\nThe reason Florence is famous is that in 1450, it was New York.\nIn 1450 it was filled with the kind of turbulent and ambitious\npeople you find now in America.  (So I went back to America.)It is greatly to America's advantage that it is\na congenial atmosphere for the right sort of unruliness\u2014that\nit is a home not just for the smart, but for smart-alecks.\nAnd hackers are invariably smart-alecks.  If we had a national\nholiday, it would be April 1st.  It says a great deal about\nour work that we use the same word for a brilliant or a\nhorribly cheesy solution.   When we cook one up we're not\nalways 100% sure which kind it is.  But as long as it has\nthe right sort of wrongness, that's a promising sign.\nIt's odd that people\nthink of programming as precise and methodical.  Computers\nare precise and methodical.  Hacking is something you do\nwith a gleeful laugh.In our world some of the most characteristic solutions\nare not far removed from practical\njokes.  IBM was no doubt rather surprised by the consequences\nof the licensing deal for DOS, just as the hypothetical\n\"adversary\" must be when Michael Rabin solves a problem by\nredefining it as one that's easier to solve.Smart-alecks have to develop a keen sense of how much they\ncan get away with.  And lately hackers \nhave sensed a change\nin the atmosphere.\nLately hackerliness seems rather frowned upon.To hackers the recent contraction in civil liberties seems\nespecially ominous.  That must also mystify outsiders. \nWhy should we care especially about civil\nliberties?  Why programmers, more than\ndentists or salesmen or landscapers?Let me put the case in terms a government official would appreciate.\nCivil liberties are not just an ornament, or a quaint\nAmerican tradition.  Civil liberties make countries rich.\nIf you made a graph of\nGNP per capita vs. civil liberties, you'd notice a definite\ntrend.  Could civil liberties really be a cause, rather\nthan just an effect?  I think so.  I think a society in which\npeople can do and say what they want will also tend to\nbe one in which the most efficient solutions win, rather than\nthose sponsored by the most influential people.\nAuthoritarian countries become corrupt;\ncorrupt countries become poor; and poor countries are weak. \nIt seems to me there is\na Laffer curve for government power, just as for\ntax revenues.  At least, it seems likely enough that it\nwould be stupid to try the experiment and find out.  Unlike\nhigh tax rates, you can't repeal totalitarianism if it\nturns out to be a mistake.This is why hackers worry.  The government spying on people doesn't\nliterally make programmers write worse code.  It just leads\neventually to a world in which bad ideas win.  And because\nthis is so important to hackers, they're especially sensitive\nto it.  They can sense totalitarianism approaching from a\ndistance, as animals can sense an approaching  \nthunderstorm.It would be ironic if, as hackers fear, recent measures\nintended to protect national security and intellectual property\nturned out to be a missile aimed right at what makes   \nAmerica successful.  But it would not be the first time that\nmeasures taken in an atmosphere of panic had\nthe opposite of the intended effect.There is such a thing as Americanness.\nThere's nothing like living abroad to teach you that.   \nAnd if you want to know whether something will nurture or squash\nthis quality, it would be hard to find a better focus\ngroup than hackers, because they come closest of any group\nI know to embodying it.  Closer, probably,  than\nthe men running our government,\nwho for all their talk of patriotism\nremind me more of Richelieu or Mazarin\nthan Thomas Jefferson or George Washington.When you read what the founding fathers had to say for\nthemselves, they sound more like hackers.\n\"The spirit of resistance to government,\"\nJefferson wrote, \"is so valuable on certain occasions, that I wish\nit always to be kept alive.\"Imagine an American president saying that today.\nLike the remarks of an outspoken old grandmother, the sayings of\nthe founding fathers have embarrassed generations of\ntheir less confident successors.  They remind us where we come from.\nThey remind us that it is the people who break rules that are\nthe source of America's wealth and power.Those in a position to impose rules naturally want them to be\nobeyed.  But be careful what you ask for. You might get it.Thanks to Ken Anderson, Trevor Blackwell, Daniel Giffin, \nSarah Harlin,  Shiro Kawai, Jessica Livingston, Matz, \nJackie McDonough, Robert Morris, Eric Raymond, Guido van Rossum,\nDavid Weinberger, and\nSteven Wolfram for reading drafts of this essay.\n(The image shows Steves Jobs and Wozniak \nwith a \"blue box.\"\nPhoto by Margret Wozniak. Reproduced by permission of Steve\nWozniak.)July 2010What hard liquor, cigarettes, heroin, and crack have in common is\nthat they're all more concentrated forms of less addictive predecessors.\nMost if not all the things we describe as addictive are.  And the\nscary thing is, the process that created them is accelerating.We wouldn't want to stop it.  It's the same process that cures\ndiseases: technological progress.  Technological progress means\nmaking things do more of what we want.  When the thing we want is\nsomething we want to want, we consider technological progress good.\nIf some new technique makes solar cells x% more efficient, that\nseems strictly better.  When progress concentrates something we\ndon't want to want\u2014when it transforms opium into heroin\u2014it seems\nbad.  But it's the same process at work.\n[1]No one doubts this process is accelerating, which means increasing\nnumbers of things we like will be transformed into things we like\ntoo much.\n[2]As far as I know there's no word for something we like too much.\nThe closest is the colloquial sense of \"addictive.\" That usage has\nbecome increasingly common during my lifetime.  And it's clear why:\nthere are an increasing number of things we need\n\nRead the document and answer: What is the best thing to do in San Francisco? Here is the most relevant sentence in the context:"}]